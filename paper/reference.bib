@article{kolamunnaDronePrintAcousticSignatures2021,
  title = {{{DronePrint}}: {{Acoustic Signatures}} for {{Open-set Drone Detection}} and {{Identification}} with {{Online Data}}},
  shorttitle = {{{DronePrint}}},
  author = {Kolamunna, Harini and Dahanayaka, Thilini and Li, Junye and Seneviratne, Suranga and Thilakaratne, Kanchana and Zomaya, Albert Y. and Seneviratne, Aruna},
  year = {2021},
  month = mar,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {5},
  number = {1},
  pages = {20:1--20:31},
  doi = {10.1145/3448115},
  urldate = {2022-04-02},
  abstract = {With the ubiquitous availability of drones, they are adopted benignly in multiple applications such as cinematography, surveying, and legal goods delivery. Nonetheless, they are also being used for reconnaissance, invading personal or secure spaces, harming targeted individuals, smuggling drugs and contraband, or creating public disturbances. These malicious or improper use of drones can pose significant privacy and security threats in both civilian and military settings. Therefore, it is vital to identify drones in different environments to assist the decisions on whether or not to contain unknown drones. While there are several methods proposed for detecting the presence of a drone, they have limitations when it comes to low visibility, limited access, or hostile environments. In this paper, we propose DronePrint that uses drone acoustic signatures to detect the presence of a drone and identify the make and the model of the drone. We address the shortage of drone acoustic data by relying on audio components of online videos. In drone detection, we achieved 96\% accuracy in a closed-set scenario, and 86\% accuracy in a more challenging open-set scenario. Our proposed method of cascaded drone identification, where a drone is identified for its 'make' followed by the 'model' of the drone achieves 90\% overall accuracy. In this work, we cover 13 commonly used commercial and consumer drone models, which is to the best of understanding is the most comprehensive such study to date. Finally, we demonstrate the robustness of DronePrint to drone hardware modifications, Doppler effect, varying SNR conditions, and in realistic open-set acoustic scenes.},
  keywords = {Acoustic fingerprinting,Drone Audio Dataset,Drones,LSTM}
}

@article{hagstrumAutomatedMonitoringUsing1996,
  title = {Automated {{Monitoring Using Acoustical Sensors}} for {{Insects}} in {{Farm-Stored Wheat}}},
  author = {Hagstrum, David W. and Flinn, Paul W. and Shuman, Dennis},
  year = {1996},
  month = feb,
  journal = {Journal of Economic Entomology},
  volume = {89},
  number = {1},
  pages = {211--217},
  issn = {0022-0493},
  doi = {10.1093/jee/89.1.211},
  urldate = {2023-01-28},
  abstract = {An automated method using cables with acoustical sensors was compared with the conventional grain sampling method for monitoring insect populations in wheat stored in 1 or 2 bins on each of 6 farms in Kansas. Seven flexible cables, each with 20 sensors 15 cm apart, were installed vertically in the grain mass along a transect across the diameter of the bin. A computer collected and stored the data. The automated system detected insects in all of the bins in which insects were found in grain samples and provided a good estimate of infestation level. Lesser grain beetle, Rhyzopertha dominica (F.) was the dominant species, and red flour beetle, Tribolium castaneum (Herbst), and rice weevil, Sitophilus oryzae (L.) were also detected. The number of times that insect sounds were detected was correlated with insect density in grain samples over a density range of 0-17 insects per kilogram. Insects were most abundant in the top center of the wheat stored in bins, and only a few sensors in this area were needed for early detection. Insects were found in grain samples at 5-37 sensor locations in any bin, but locations differed between bins. Insects were found in grain samples at a total of 50 sensor locations in 10 bins. Sensors will need to be distributed over a representative portion of a bin to determine insect infestation levels accurately.},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\HEM6BA9M\\Hagstrum et al. - 1996 - Automated Monitoring Using Acoustical Sensors for .pdf;C\:\\Users\\daniel\\Zotero\\storage\\QCQ3KVDF\\Hagstrum et al. - 1996 - Automated Monitoring Using Acoustical Sensors for .pdf;C\:\\Users\\daniel\\Zotero\\storage\\H44Q994D\\2962168.html}
}

@article{mankinPerformanceLowCostAcoustic2020,
  title = {Performance of a {{Low-Cost Acoustic Insect Detector System}} with {{Sitophilus Oryzae}} (Coleoptera: {{Curculionidae}}) in {{Stored Grain}} and {{Tribolium Castaneum}} (Coleoptera: {{Tenebrionidae}}) in {{Flour}}},
  shorttitle = {Performance of a {{Low-Cost Acoustic Insect Detector System}} with {{Sitophilus Oryzae}} (Coleoptera},
  author = {Mankin, R W and Jetter, E and Rohde, B and Yasir, M},
  year = {2020},
  month = dec,
  journal = {Journal of Economic Entomology},
  volume = {113},
  number = {6},
  pages = {3004--3010},
  issn = {0022-0493},
  doi = {10.1093/jee/toaa203},
  urldate = {2023-01-28},
  abstract = {Reduction of postharvest losses is gaining increased priority in warm regions where insect infestation may cause rapid deterioration of staple commodities. Acoustic detection can be used to assess the likelihood of insect infestations in bags of grain, flour, and other commodities that are stored in small holdings in developing countries, enabling rapid targeting of treatments. A portable postharvest insect detection system was developed with the goal to provide low-cost capability to acoustically assess infestations in small-scale storage facilities. Electret microphones input pest insect sounds to a 32-bit microcontroller platform that digitized and stored the signals on a digital memory card transferable to a portable laptop computer. The insect sounds then were analyzed by custom-written software that matched their spectra to those of known pests. Infestations of Sitophilus oryzae (L) in 2.6-kg bags could be detected down to densities of 1.9 adults/kg in grain and Tribolium castaneum (Herbst) down to 3.8 adults/kg in flour in laboratory settings. Also, differences in the rates of sounds per insect in treatments with different numbers ranging from 5 to 50 insects suggested that the sound rates of adults of different species at different population densities may be noticeably affected by aggregation pheromones or other behaviorally active semiochemicals. Further testing is needed but previous experience with acoustic detection systems suggests that the prototype has potential for use in small storage facilities where early detection of infestations is difficult to provide.},
  keywords = {Acoustics,aggregation pheromones,Animals,automatic detection,bags,Coleoptera,computer software,detection limit,detectors,equipment performance,flour,Flour,food security,grain storage facilities,imagos,infestation likelihood,insect infestations,Insecta,microphones,population density,portable equipment,postharvest,Sitophilus oryzae,sounds,storage insects,stored grain,stored product protection,Tribolium,Tribolium castaneum,Weevils},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\I5NAQDTK\\Mankin et al. - 2020 - Performance of a Low-Cost Acoustic Insect Detector.pdf;C\:\\Users\\daniel\\Zotero\\storage\\P9K866SD\\Mankin et al. - 2020 - Performance of a Low-Cost Acoustic Insect Detector.pdf;C\:\\Users\\daniel\\Zotero\\storage\\UBWWUMRD\\5910423.html}
}

@article{mankinNoiseShieldingAcoustic1996,
  title = {Noise {{Shielding}} of {{Acoustic Devices}} for {{Insect Detection}}},
  author = {Mankin, R. W. and Shuman, D. and Coffelt, J. A.},
  year = {1996},
  month = oct,
  journal = {Journal of Economic Entomology},
  volume = {89},
  number = {5},
  pages = {1301--1308},
  issn = {0022-0493},
  doi = {10.1093/jee/89.5.1301},
  urldate = {2023-02-01},
  abstract = {The need to monitor hidden insects and automate the acquisition of data for grain management has led to development of electronic sound detection devices. Typically, insect feeding and movement sounds are low in intensity, and they attenuate rapidly in grain. The mean sound pressure level (SPL) generated by rice weevil, Sitophilus oryzae (L.) (Coleoptera: Curculionidae), larvae in wheat kernels is only 23 dB (referenced to 20 {$\mu$}Pa) as measured by a microphone immersed in a grain sample 3 cm from a larva-infested kernel. Unless the noise background is reduced below these levels by 10 dB or more, an insect cannot be detected reliably. To establish guidelines and procedures for shielding acoustic detectors in a grain elevator or other noisy environment, a multi layered enclosure was constructed that attenuates sound by 70--85 dB between 1 and 10 kHz. This level of noise reduction is sufficient to enable detection of internally feeding larvae in grain samples at inspection facilities at commercial grain elevators, which have 50--80 dB SPL noise backgrounds between 0.1 and 10 kHz.},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\4L45WTQ7\\Mankin et al. - 1996 - Noise Shielding of Acoustic Devices for Insect Det.pdf;C\:\\Users\\daniel\\Zotero\\storage\\4BCLJNUE\\791655.html}
}

@article{mankinAutomatedApplicationsAcoustics2021,
  title = {Automated {{Applications}} of {{Acoustics}} for {{Stored Product Insect Detection}}, {{Monitoring}}, and {{Management}}},
  author = {Mankin, Richard and Hagstrum, David and Guo, Min and Eliopoulos, Panagiotis and Njoroge, Anastasia},
  year = {2021},
  month = mar,
  journal = {Insects},
  volume = {12},
  number = {3},
  pages = {259},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2075-4450},
  doi = {10.3390/insects12030259},
  urldate = {2023-02-02},
  abstract = {Acoustic technology provides information difficult to obtain about stored insect behavior, physiology, abundance, and distribution. For example, acoustic detection of immature insects feeding hidden within grain is helpful for accurate monitoring because they can be more abundant than adults and be present in samples without adults. Modern engineering and acoustics have been incorporated into decision support systems for stored product insect management, but with somewhat limited use due to device costs and the skills needed to interpret the data collected. However, inexpensive modern tools may facilitate further incorporation of acoustic technology into the mainstream of pest management and precision agriculture. One such system was tested herein to describe Sitophilus oryzae (Coleoptera: Curculionidae) adult and larval movement and feeding in stored grain. Development of improved methods to identify sounds of targeted pest insects, distinguishing them from each other and from background noise, is an active area of current research. The most powerful of the new methods may be machine learning. The methods have different strengths and weaknesses depending on the types of background noise and the signal characteristic of target insect sounds. It is likely that they will facilitate automation of detection and decrease costs of managing stored product insects in the future.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {<i>Sitophilus oryzae</i>,<i>Tribolium castaneum</i>,abundance,machine learning,neural networks,population density,Sitophilus oryzae,Tribolium castaneum},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\443RZYFR\\Mankin et al. - 2021 - Automated Applications of Acoustics for Stored Pro.pdf;C\:\\Users\\daniel\\Zotero\\storage\\GPQ3CWJG\\Mankin et al. - 2021 - Automated Applications of Acoustics for Stored Pro.pdf;C\:\\Users\\daniel\\Zotero\\storage\\M8ACCUEG\\Mankin et al. - 2021 - Automated Applications of Acoustics for Stored Pro.pdf;C\:\\Users\\daniel\\Zotero\\storage\\RKZGJ2PI\\Mankin et al. - 2021 - Automated Applications of Acoustics for Stored Pro.pdf;C\:\\Users\\daniel\\Zotero\\storage\\XRI77VEN\\Mankin et al. - 2021 - Automated Applications of Acoustics for Stored Pro.pdf;C\:\\Users\\daniel\\Zotero\\storage\\ZPWFC65E\\Mankin et al. - 2021 - Automated Applications of Acoustics for Stored Pro.pdf}
}

@article{mankinAcousticIndicatorsTargeted2010,
  title = {Acoustic {{Indicators}} for {{Targeted Detection}} of {{Stored Product}} and {{Urban Insect Pests}} by {{Inexpensive Infrared}}, {{Acoustic}}, and {{Vibrational Detection}} of {{Movement}}},
  author = {Mankin, R. W. and Hodges, R. D. and Nagle, H. T. and Schal, C. and Pereira, R. M. and Koehler, P. G.},
  year = {2010},
  month = oct,
  journal = {Journal of Economic Entomology},
  volume = {103},
  number = {5},
  pages = {1636--1646},
  issn = {0022-0493},
  doi = {10.1603/EC10126},
  urldate = {2023-02-02},
  abstract = {Crawling and scraping activity of three stored-product pests, Sitophilus oryzae (L.) (Coleoptera: Curculionidae), Tribolium castaneum (Herbst) (Coleoptera: Tenebrionidae), and Stegobium paniceum (L.) (Coleoptera: Anobiidae), and two urban pests, Blattella germanica (L.) (Blattodea: Blattellidae) and Cimex lectularius L. (Hemiptera: Cimicidae), were monitored individually by infrared sensors, microphones, and a piezoelectric sensor in a small arena to evaluate effects of insect locomotory behavior and size on the ability of an inexpensively constructed instrument to detect insects and distinguish among different species. Adults of all species could be detected when crawling or scraping. The smallest insects in the study, first-fourth-instar C. lectularius nymphs, could not be detected easily when crawling, but could be detected when scraping. Sound and vibration sensors detected brief, 3-10-ms impulses from all tested species, often grouped in distinctive trains (bursts), typical of impulses in previous acoustic detection experiments. To consider the potential for targeting or focusing detection on particular species of interest, indicators were developed to assess the likelihood of detection of C. lectularius. Statistically significant differences were found between C. lectularius and other species in distributions of three measured variables: infrared signal durations, sound impulse-burst durations, and sound pressure levels (energy) of impulses that best matched an averaged spectrum (profile) of scraping behavior. Thus, there is potential that signals collected by an inexpensive, polymodal-sensor instrument could be used in automated trapping systems to detect a targeted species, 0.1 mg or larger, in environments where servicing of traps is difficult or when timeliness of trapping information is important.},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\8HT792CT\\Mankin et al. - 2010 - Acoustic Indicators for Targeted Detection of Stor.pdf;C\:\\Users\\daniel\\Zotero\\storage\\QJA4CNBL\\Mankin et al. - 2010 - Acoustic Indicators for Targeted Detection of Stor.pdf;C\:\\Users\\daniel\\Zotero\\storage\\8HECJ5QM\\790644.html}
}

@article{mankinPerspectivePromiseCentury2011,
  title = {Perspective and {{Promise}}: A {{Century}} of {{Insect Acoustic Detection}} and {{Monitoring}}},
  shorttitle = {Perspective and {{Promise}}},
  author = {Mankin, R. W. and Hagstrum, D. W. and Smith, M. T. and Roda, A. L. and Kairo, M. T. K.},
  year = {2011},
  month = jan,
  journal = {American Entomologist},
  volume = {57},
  number = {1},
  pages = {30--44},
  issn = {1046-2821},
  doi = {10.1093/ae/57.1.30},
  urldate = {2023-02-02},
  abstract = {Acoustic devices provide nondestructive, remote, automated detection, and monitoring of hidden insect infestations for pest managers, regulators, and researchers. In recent decades, acoustic devices of various kinds have been marketed for field use, and instrumented sample containers in sound-insulated chambers have been developed for commodity inspection. The efficacy of acoustic devices in detecting cryptic insects, estimating population density, and mapping distributions depends on many factors, including the sensor type and frequency range, the substrate structure, the interface between sensor and substrate, the assessment duration, the size and behavior of the insect, and the distance between the insects and the sensors. Considerable success has been achieved in detecting grain and wood insect pests. Microphones are useful sensors for airborne signals, but vibration sensors interface better with signals produced in solid substrates, such as soil, grain, or fibrous plant structures. Ultrasonic sensors are particularly effective for detecting wood-boring pests because background noise is negligible at \&gt; 20 kHz frequencies, and ultrasonic signals attenuate much less rapidly in wood than in air; grain, or soil. Problems in distinguishing sounds produced by target species from other sounds have hindered usage of acoustic devices, but new devices and signal processing methods have greatly increased the reliability of detection. One new method considers spectral and temporal pattern features that prominently appear in insect sounds but not in background noise, and vice versa. As reliability and ease of use increase and costs decrease, acoustic devices have considerable future promise as cryptic insect detection and monitoring tools.},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\46J8DYFN\\Mankin et al. - 2011 - Perspective and Promise a Century of Insect Acous.pdf;C\:\\Users\\daniel\\Zotero\\storage\\6R32NPF5\\Mankin et al. - 2011 - Perspective and Promise a Century of Insect Acous.pdf;C\:\\Users\\daniel\\Zotero\\storage\\TIVP5TSM\\Mankin et al. - 2011 - Perspective and Promise a Century of Insect Acous.pdf;C\:\\Users\\daniel\\Zotero\\storage\\W7LXJY22\\Mankin et al. - 2011 - Perspective and Promise a Century of Insect Acous.pdf;C\:\\Users\\daniel\\Zotero\\storage\\YZ8B428B\\Mankin et al. - 2011 - Perspective and Promise a Century of Insect Acous.pdf;C\:\\Users\\daniel\\Zotero\\storage\\EFPUHQ6H\\2462094.html}
}

@article{mankinRecentDevelopmentsUse2011,
  title = {Recent {{Developments}} in the {{Use}} of {{Acoustic Sensors}} and {{Signal Processing Tools}} to {{Target Early Infestations}} of {{Red Palm Weevil}} in {{Agricultural Environments}}},
  author = {Mankin, Richard W.},
  year = {2011},
  journal = {The Florida Entomologist},
  volume = {94},
  number = {4},
  eprint = {23065826},
  eprinttype = {jstor},
  pages = {761--765},
  publisher = {Florida Entomological Society},
  issn = {0015-4040},
  urldate = {2023-02-02},
  abstract = {Much of the damage caused by red palm weevil larvae to date palms, ornamental palms, and palm offshoots could be mitigated by early detection and treatment of infestations. Acoustic technology has potential to enable early detection, but the short, high-frequency sound impulses produced by red palm weevil larvae can be difficult to distinguish from certain similar sounds produced by other insects or small animals, or by wind-induced tapping noises. Considerable research has been conducted to develop instruments and signal processing software that selectively amplify insect-produced sounds and identify signal features that distinguish sounds produced by a particular target insect from those produced by other causes. Progress has been made in identifying unique spectral and temporal patterns in the sounds produced by larvae during movement and feeding activities. This report describes some of the new instrumentation and signal analyses available for early, reliable detection of red palm weevil larvae in groves and greenhouses. Gran parte de los da{\~n}os causados por las larvas del gorgojo rojo de las palmas a las palmeras d{\'a}tiles, palmas ornamentales y los reto{\~n}os de palmas podr{\'i}an ser mitigados mediante la detecci{\'o}n temprana y el tratamiento de las infestaciones. La tecnolog{\'i}a ac{\'u}stica tiene el potencial de permitir la detecci{\'o}n temprana, pero los impulsos cortos de alta frecuencia de sonido producidas por las larvas del gorgojo rojo de las palmas pueden ser dif{\'i}ciles de distinguir de ciertos sonidos similares producidos por otros insectos o animales peque{\~n}os o los toques de sonido producidos por el viento. Se han realizado muchas investigaciones para desarrollar instrumentos y software (programas de computadora) de procesamiento de se{\~n}ales que amplifican selectivamente los sonidos producidos por insectos e identificar las caracter{\'i}sticas de las se{\~n}ales que distingue a los sonidos producidos por el insecto clave en particular de los producidos por otras causas. Se ha avanzado en la identificaci{\'o}n de patrones espectrales y temporales {\'u}nicos de los sonidos producidos por las larvas durante las actividades de movimiento y la alimentaci{\'o}n. Este informe describe algunos de los nuevos instrumentos y an{\'a}lisis de se{\~n}ales para la detecci{\'o}n temprana y fiable de las larvas del gorgojo rojo de las palmas en las plantaciones e invernaderos.},
  file = {C:\Users\daniel\Zotero\storage\AFCZTRMY\Mankin - 2011 - Recent Developments in the Use of Acoustic Sensors.pdf}
}

@inproceedings{salloumLowcostMultimodalIntegrated2022,
  title = {Low-Cost Multimodal Integrated Marine Surveillance System},
  booktitle = {2022 {{IEEE International Symposium}} on {{Technologies}} for {{Homeland Security}} ({{HST}})},
  author = {Salloum, Hady and Sutin, Alexander and Sedunov, Nikolay and Sedunov, Alexander and Kadyrov, Daniel},
  year = {2022},
  month = nov,
  pages = {1--6},
  doi = {10.1109/HST56032.2022.10025450},
  abstract = {Detection of surface vessels, semisubmersibles, and underwater vehicles is required for several Maritime Law Enforcement missions, including drug and alien migrant interdiction, monitoring, control, and surveillance of illegal, unregulated, and unreported (IUU) fishing, as well as protection from maritime terrorism. Detection and monitoring of vessels involved in illegal activity occurs principally through the collection, analysis, and dissemination of tactical information and strategic intelligence combined with effective sensors operating from land, air, and surface assets. Stevens Institute of Technology (SIT) built and tested an experimental low-cost sensor suite dubbed the Boat Detection System (BDS) prototype that can work autonomously on the shore or at sea using available platforms. The suggested low-cost automated sensor system costs less than current land and air-based sensors and does not require a human in the loop for its operation. The experimental sensor suite uses low-cost COTS sensors including marine radar, optical and infrared cameras, and AIS receivers in conjunction with an underwater acoustic array, the Stevens Passive Acoustic System (SPADES-2) prototype, outfitted with Stevens custom-made lowcost hydrophones.},
  copyright = {All rights reserved},
  keywords = {Boats,Laser radar,maritime surveillance,Passive radar,Prototypes,radar,Sea surface,small boats,Sonar equipment,Surveillance,underwater acoustics},
  file = {C:\Users\daniel\Zotero\storage\I6MZIWFM\10025450.html}
}

@article{neethirajanDetectionTechniquesStoredProduct2007,
  title = {Detection {{Techniques}} for {{Stored-Product Insects}} in {{Grain}}},
  author = {Neethirajan, S. and Karunakaran, C. and Jayas, D.S. and White, N.D.G.},
  year = {2007},
  month = feb,
  journal = {Food Control},
  volume = {18},
  number = {2},
  pages = {157--162},
  issn = {09567135},
  doi = {10.1016/j.foodcont.2005.09.008},
  urldate = {2023-02-28},
  abstract = {Cereal grains are the major source of food for humans and most domesticated animals. In many developing countries, overall postharvest losses of cereals and legumes of about 10--15\% are fairly common. Consumption of cereals and legumes by pests such as insects during storage and microbial spoilage or contamination may make these totally inedible. On farms, manual samples, traps, and probes have been used to determine the presence of insects. Manual inspection, sieving, cracking-Xoatation and Berlese funnels are being used at present to detect insects in grain handling facilities. These methods are not eYcient and are time consuming. Acoustic detection, carbon dioxide measurement, uric acid measurement, near-infrared spectroscopy, and soft X-ray method have the potential for use at the industry level to detect insects in grain samples as their usefulness has been demonstrated in the research laboratories. Researchers have developed image analysis programs to automatically scan X-ray images to detect insect infestations. The use of near-infrared (NIR) spectroscopy has been investigated to detect hidden insects in wheat kernels. X-ray and NIR spectroscopy methods are cost prohibitive and current NIR instrumentation requires complex operating procedures and calibrations. The advantages and limitations of these insect detection methods are evaluated and the advantages of one technique over the other are described in this paper.},
  langid = {english},
  keywords = {Acoustical method,Berlese funnel,Electrical conductance,Insect detection,Insect trap,Machine vision,NIR spectroscopy,Pheromones,X-ray imaging},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\EU3TZP3K\\Neethirajan et al. - 2007 - Detection techniques for stored-product insects in.pdf;C\:\\Users\\daniel\\Zotero\\storage\\PE2X6E4R\\Neethirajan et al. - 2007 - Detection Techniques for Stored-Product Insects in.pdf;C\:\\Users\\daniel\\Zotero\\storage\\W32FTF8N\\Neethirajan et al. - 2007 - Detection techniques for stored-product insects in.pdf;C\:\\Users\\daniel\\Zotero\\storage\\C3V4XFKT\\S0956713505002161.html}
}

@article{shumanQuantitativeAcousticalDetection1993,
  title = {Quantitative {{Acoustical Detection}} of {{Larvae Feeding Inside Kernels}} of {{Grain}}},
  author = {Shuman, Dennis and Coffelt, James A. and Vick, Kenneth W. and Mankin, Richard W.},
  year = {1993},
  month = jun,
  journal = {Journal of Economic Entomology},
  volume = {86},
  number = {3},
  pages = {933--938},
  issn = {0022-0493},
  doi = {10.1093/jee/86.3.933},
  urldate = {2023-02-28},
  abstract = {An automated, computer-based electronic acoustic system was developed to quantify infestation of internally feeding larvae in a grain sample using spatial localization of insects in the sample. Localization was determined using arrival times of sounds produced by insect feeding activity as received by an array of acoustic transducers. In a test conducted with 0-3 fourth instars of the rice weevil, Sitophilus oryzae (L.), in 1-kg samples of wheat, the system overassessed the number of larvae present in 6\% of the trials and underassessed the number of larvae present in 34\% of the trials. When Federal Grain Inspection Service (FGIS) standards were applied in evaluating performance, the system was 92\% accurate in grading ``clean'' grain and 64\% accurate in grading ``infested'' grain.},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\BTCRJAAX\\Shuman et al. - 1993 - Quantitative Acoustical Detection of Larvae Feedin.pdf;C\:\\Users\\daniel\\Zotero\\storage\\RMVTXDFR\\2216065.html}
}

@article{bangaTechniquesInsectDetection2018,
  title = {Techniques for {{Insect Detection}} in {{Stored Food Grains}}: {{An Overview}}},
  shorttitle = {Techniques for {{Insect Detection}} in {{Stored Food Grains}}},
  author = {Banga, Km Sheetal and Kotwaliwale, Nachiket and Mohapatra, Debabandya and Giri, Saroj Kumar},
  year = {2018},
  month = dec,
  journal = {Food Control},
  volume = {94},
  pages = {167--176},
  issn = {0956-7135},
  doi = {10.1016/j.foodcont.2018.07.008},
  urldate = {2023-02-28},
  abstract = {Insects cause a major loss in stored food grains. Besides, pestilential activities of insects in stored food grains affect the marketability as well as the nutritional values. Early detection and monitoring of insects in the stored food grains become necessary for applying corrective actions. Visual inspection, probe sampling, insect trap, Berlese funnel, visual lures, pheromone devices etc., are some of the popular methods largely used in commercial granaries or grain storage establishments. Of late, electronic nose, solid phase micro-extraction, thermal imaging, acoustic detection, etc. have been reported to be successful in detecting insects. The capability of in-situ early detection, monitoring, cost, reliability, and labor requirements are the major factors considered during for selection of the method. Detection of hidden infestation, whose population may be many times higher than the free-living insects is an important concern to mitigate the losses in bulk storage warehouses, so as to enable the early actions for fumigation or to dispose off the grain. This paper reviews some of the widely used detection methods for early detection of insects' pestilential activities in stored food grains as well as some of the novel technologies with an emphasis on acoustic method, which has a good commercial potential.},
  langid = {english},
  keywords = {Acoustic detection,Detection method,Electronic nose,Pestilential,Post-harvest losses},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\3BRADZ8C\\Banga et al. - 2018 - Techniques for Insect Detection in Stored Food Gra.pdf;C\:\\Users\\daniel\\Zotero\\storage\\CXDYFG8E\\Banga et al. - 2018 - Techniques for Insect Detection in Stored Food Gra.pdf;C\:\\Users\\daniel\\Zotero\\storage\\TXCETLN8\\Banga et al. - 2018 - Techniques for insect detection in stored food gra.pdf;C\:\\Users\\daniel\\Zotero\\storage\\IB54328Y\\S0956713518303372.html}
}

@article{t.pearsonDetectionWheatKernels2007,
  title = {Detection of {{Wheat Kernels}} with {{Hidden Insect Infestations}} with an {{Electrically Conductive Roller Mill}}},
  author = {{T. Pearson} and {D. L. Brabec}},
  year = {2007},
  journal = {Applied Engineering in Agriculture},
  volume = {23},
  number = {5},
  pages = {639--645},
  issn = {1943-7838},
  doi = {10.13031/2013.23662},
  urldate = {2023-02-28},
  abstract = {A laboratory roller mill system was modified to measure and analyze the electrical conductance of wheat as it was crushed. The electrical conductance of normal wheat kernels is normally low and fairly constant. In contrast, the electrical conductance of wheat kernels infested with live insects is substantially higher, depending on the size of the larvae and the resulting contact of the crushed larvae between the rolls. This instrument was designed to detect internal insect infestations in wheat that has a moisture content of 13.5\% or less. The laboratory mill can test a kilogram (kg) of wheat in less than 2 min and 100 g in less than 10 s. Hard red winter and soft red winter wheat containing larvae of rice weevils and lesser grain borers of a variety of sizes were tested. On average, the instrument detected 8.3 out of 10 infested kernels per 100 g of wheat containing the larger sized insects (fourth instar or pupae). Kernels infested with medium-sized larvae (second or third instar) were detected at an average rate of 7.4 out of 10 infested kernels per 100 g of wheat. Finally, kernels infested with the small-sized larvae (first or second instar) were detected at a rate of 5.9 out of 10 infested kernels per 100 g of wheat. Under reasonable grain moisture contents there were no false positive errors, or noninfested kernels classified as insect infested. The cost of the mill is low and can lead to rapid and automated detection of infested wheat.},
  langid = {english},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\7NM7T25P\\T. Pearson and D. L. Brabec - 2007 - Detection of Wheat Kernels with Hidden Insect Infe.pdf;C\:\\Users\\daniel\\Zotero\\storage\\KFQUVU2N\\T. Pearson and D. L. Brabec - 2007 - Detection of Wheat Kernels with Hidden Insect Infe.pdf;C\:\\Users\\daniel\\Zotero\\storage\\P8R2S85F\\T. Pearson and D. L. Brabec - 2007 - Detection of Wheat Kernels with Hidden Insect Infe.pdf}
}

@article{mohanToolsStoredProduct2016,
  title = {Tools for {{Stored Product Insects Management}} and {{Technology Transfer}}},
  author = {Mohan, s and Abburi, Rajesh},
  year = {2016},
  month = jan,
  journal = {Indian Journal of Entomology},
  volume = {78},
  pages = {59},
  doi = {10.5958/0974-8172.2016.00026.2},
  abstract = {Food grains are stored in farm, home or warehouses and insects continue to create problems by causing qualitative and quantitative damage in storage. This necessitates the use of tools for early detection (probe trap, pit fall trap and stack probe trap- Indian Patent Application No.1733/CHE/2008, dt.24.7.2008, early adult removal (Insect removal bins), or for control (Insect egg removal device- Indian Patent No. 198434, and UV Light trap). These tools have become essential in the current scenario of ``Global Food security''. Attempts were made to transfer these tools namely viz., TNAU insect probe trap, Pitfall trap, Two-in-one trap, Indicator device, Automatic insect removal bin, Egg remover device, Stack probe trap and UV Light trap for warehouse, to end users in a systematic and scientific way following the basic principles of agricultural extension. The methods used were by developing entrepreneurs for continuous supply of the tools, feedback studies in various states of India, by conducting training to end users and reaching students of agricultural colleges through developing a ``Educational Kit'' with multimedia, hosting the trapping devices on many websites and publishing details of the tools in popular magazines. A significant impact has resulted because of these exercises in terms of end users of the technologies.}
}

@article{epskyLaboratoryEvaluationImproved2001,
  title = {Laboratory {{Evaluation}} of an {{Improved Electronic Grain Probe Insect Counter}}},
  author = {Epsky, Nancy D and Shuman, Dennis},
  year = {2001},
  month = apr,
  journal = {Journal of Stored Products Research},
  volume = {37},
  number = {2},
  pages = {187--197},
  issn = {0022-474X},
  doi = {10.1016/S0022-474X(00)00019-9},
  urldate = {2023-02-28},
  abstract = {An Electronic Grain Probe Insect Counter system, which incorporates modified passive grain probes, allows offsite monitoring and detection of insect pests in stored grain. An electronic count is generated whenever an insect falls through an infrared beam in the sensor head located at the bottom of the electronic grain probe. We report descriptions and laboratory evaluations of prototype electronic grain probes that were custom-made in-house (n=8) and by small-scale manufacturing (n=54). Laboratory tests, in which dead insects were dropped through a probe, were conducted to determine if electronic probes accurately count the numbers of insects that are captured. Accuracy of the manufactured electronic probes increased as the size of the test insect increased from 93.6\% for the smallest insect tested (Cryptolestes ferrugineus, the rusty grain beetle) to 99.5\% for the largest (Tribolium castaneum, the red flour beetle). Custom-made probes were significantly more accurate for C. ferrugineus (96.5\% versus 93.6\%) but there was no difference in accuracies for the larger insects. Comparisons among all probes found that probe accuracy was correlated with variation in the magnitude of the output signal from the infrared phototransistor. Thus, use of diode/phototransistor pairs with a more consistent beam or with improved beam focus may further improve probe accuracy. Good performance was obtained with the manufactured electronic probes. Tests with live insects under field conditions are needed to further evaluate the system performance.},
  langid = {english},
  keywords = {Automated monitoring,Infrared sensor},
  file = {C:\Users\daniel\Zotero\storage\V9QEBC3D\S0022474X00000199.html}
}

@article{rajeshEfficacyTnauStackProbe2015,
  title = {Efficacy of {{Tnau-Stack Probe Trap}} in {{Detection}}, {{Monitoring}} of {{Pests}} and {{Validation}} of {{Fumigation}} in {{Turmeric Bag-Stack Storage}}.},
  author = {Rajesh, A. and Mohan, S. and Nelson, S. J.},
  year = {2015},
  journal = {Current Biotica},
  volume = {9},
  number = {2},
  pages = {159--165},
  publisher = {Current Biotica},
  issn = {0973-4031},
  urldate = {2023-03-01},
  abstract = {Trials were carried out to assess the efficacy of Stack probe trap in detection, monitoring of pests and to validate the post fumigation catch in turmeric bag stack storage at two locations, Coimbatore and Erode districts of Tamil Nadu, India. Different numbers of stack probe traps (4, 8, 12, 16 \&amp; 20) per bag stack (capacity of each stack {$\sim$}5 MT) were used for detection of pest infestation and...},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\5NHDCJ74\20163126510.html}
}

@inproceedings{shumanCommercializationElectronicGrain2001,
  title = {Commercialization of the {{Electronic Grain Probe Insect Counter}}},
  author = {Shuman, D. and Epsky, N.},
  year = {2001},
  urldate = {2023-03-01},
  abstract = {Technology transfer in the form of a Cooperative Research and Development Agreement (CRADA) has been entered into with OPI Systems, Inc. of Calgary, Canada, a company that produces automated stored-product management systems, to refine for manufacture ARS' proprietary Electronic Grain Probe Insect Counter (EGPIC) System (U. S. Patent No. 5,646,404). The system automatically provides information about the presence and extent of insect infestations in bulk-stored agricultural products without the necessity of entering the storage structure. EGPIC uses an array of perforated tubes (probes) distributed throughout the bulk stored product. Insects wandering in the stored product crawl into the tubes and then drop down past electronic sensors that send counts back to a central computer. Insect counts are automatically analyzed, resulting in displays of population distribution estimates. These displays provide an early warning, allowing managers to maximize the efficient usage of fumigant or controlled atmosphere control strategies. The realtime system data also provides immediate feedback on the effectiveness of the applied control measures. System refinements addressed pertain to (a) the efficient production of the sensor head, (b) the optimization of the probe body entry hole array, and (c) the modification of the EGPIC electronics to interface with the existing OPI Systems data transmission architecture. Commercialization of EGPIC will provide the agricultural industry with a safe, effective tool for monitoring insect populations, which is an essential component of any Integrated Pest Management program.}
}

@article{fantle-lepczykEconomicCostsBiological2022,
  title = {Economic {{Costs}} of {{Biological Invasions}} in the {{United States}}},
  author = {{Fantle-Lepczyk}, Jean E. and Haubrock, Phillip J. and Kramer, Andrew M. and Cuthbert, Ross N. and Turbelin, Anna J. and {Crystal-Ornelas}, Robert and Diagne, Christophe and Courchamp, Franck},
  year = {2022},
  month = feb,
  journal = {Science of The Total Environment},
  volume = {806},
  pages = {151318},
  issn = {00489697},
  doi = {10.1016/j.scitotenv.2021.151318},
  urldate = {2023-07-12},
  langid = {english},
  keywords = {Agriculture,Animals,Cost of Illness,Ecosystem,Health Care Costs,Insecta,Introduced Species,InvaCost,Invasive alien species,Non-native species,Nonindigenous species,Socioeconomic damages,United States}
}

@inproceedings{bennettTrumanScientificGuide1988,
  title = {Truman's {{Scientific Guide}} to {{Pest Control Operations}}},
  author = {Bennett, G. and Owens, J. and Corrigan, R.},
  year = {1988},
  urldate = {2023-07-12},
  abstract = {TRUMAN'S SCIENTIFIC GUIDE TO PEST CONTROL OPERATIONS by G.W. Bennett, J. M. Owens, R.M. Corrigan Updated 4th edition. Find answers to hundreds of questions about pest control. Learn sound techniques for pest management programs for hundreds of types of pests. New chapters on urban wildlife, pest control for food plants, specialized facilities and fumigation. Lavishly illustrated, fully indexed. PECO 101 \$69.95 AiTURGCON V}
}

@article{eliopoulosDetectionAdultBeetles2015,
  title = {Detection of {{Adult Beetles Inside}} the {{Stored Wheat Mass Based}} on {{Their Acoustic Emissions}}},
  author = {Eliopoulos, P. A. and Potamitis, I. and Kontodimas, D. Ch and Givropoulou, E. G.},
  year = {2015},
  month = dec,
  journal = {Journal of Economic Entomology},
  volume = {108},
  number = {6},
  pages = {2808--2814},
  publisher = {Oxford Academic},
  issn = {0022-0493},
  doi = {10.1093/jee/tov231},
  urldate = {2023-07-21},
  abstract = {Abstract. The efficacy of bioacoustics in detecting the presence of adult beetles inside the grain mass was evaluated in the laboratory. A piezoelectric sensor},
  langid = {english},
  keywords = {Animals,beetle pest,bioacoustics,Coleoptera,detection,Edible Grain,Food Parasitology,population density,Sound,stored wheat},
  file = {C:\Users\daniel\Zotero\storage\LY7F8S5Z\Eliopoulos et al. - 2015 - Detection of Adult Beetles Inside the Stored Wheat.pdf}
}

@article{baldwinConfusedFlourBeetle2020,
  title = {Confused {{Flour Beetle}}, {{Tribolium}} Confusum {{Jacquelin}} Du {{Val}} ({{Insecta}}: {{Coleoptera}}: {{Tenebrionidae}}) and {{Red Flour Beetle}}, {{Tribolium}} Castaneum ({{Herbst}}) ({{Insecta}}: {{Coleoptera}}: {{Tenebrionidae}})},
  author = {Baldwin, Rebecca and Fasulo, Thomas R},
  year = {2020},
  month = nov,
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\DYDFXDHH\Baldwin and Fasulo - 2020 - Confused Flour Beetle, Tribolium confusum Jacqueli.pdf}
}

@misc{mankinBugBytesSound2019,
  title = {Bug {{Bytes Sound Library}}: {{Stored Product Insect Pest Sounds}}},
  shorttitle = {Bug {{Bytes Sound Library}}},
  author = {Mankin, Richard},
  year = {2019},
  publisher = {Ag Data Commons},
  doi = {10.15482/USDA.ADC/1504600},
  urldate = {2023-07-22},
  langid = {english}
}

@article{vickSoundDetectionStoredProduct1988,
  title = {Sound {{Detection}} of {{Stored-Product Insects That Feed Inside Kernels}} of {{Grain}}},
  author = {Vick, K. W. and Webb, J. C. and Weaver, B. A. and Litzkow, C.},
  year = {1988},
  month = oct,
  journal = {Journal of Economic Entomology},
  volume = {81},
  number = {5},
  pages = {1489--1493},
  issn = {0022-0493},
  doi = {10.1093/jee/81.5.1489},
  urldate = {2023-07-29},
  abstract = {A system for acoustically detecting internal-feeding insect larvae in grain is described. Larvae of the lesser grain borer, Rhyzopertha dominica (F.); rice weevil, Sitophilus oryzae (L.); and Angoumois grain moth, Sitotroga cerealella (Olivier), produced sounds loud enough to be detected 13-19 d after oviposition, depending upon the species. After first detection, larvae produced detectable sounds 71-90\% of the time until pupation. Infestation rates could be estimated, at least in the range of 1-20 infested kernels per 100 ml of grain, where the infestation rate was strongly correlated to number of sounds as counted by a frequency counter.},
  file = {C:\Users\daniel\Zotero\storage\URFV9UT5\Vick et al. - 1988 - Sound Detection of Stored-Product Insects That Fee.pdf}
}

@article{singhPulsesOverview2017,
  title = {Pulses: {{An Overview}}},
  shorttitle = {Pulses},
  author = {Singh, Narpinder},
  year = {2017},
  month = mar,
  journal = {Journal of Food Science and Technology},
  volume = {54},
  number = {4},
  pages = {853--857},
  issn = {0975-8402},
  doi = {10.1007/s13197-017-2537-4},
  urldate = {2023-08-04},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\QSZ9N828\Singh - 2017 - Pulses An Overview.pdf}
}

@incollection{awikaMajorCerealGrains2011,
  title = {Major {{Cereal Grains Production}} and {{Use Around}} the {{World}}},
  booktitle = {Advances in {{Cereal Science}}: {{Implications}} to {{Food Processing}} and {{Health Promotion}}},
  author = {Awika, Joseph M.},
  year = {2011},
  month = jan,
  series = {{{ACS Symposium Series}}},
  volume = {1089},
  pages = {1--13},
  publisher = {American Chemical Society},
  doi = {10.1021/bk-2011-1089.ch001},
  urldate = {2023-08-04},
  abstract = {Cereal grains have been the principal component of human diet for thousands of years and have played a major role in shaping human civilization. Around the world, rice, wheat, and maize, and to a lesser extent, sorghum and millets, are important staples critical to daily survival of billions of people. More than 50\% of world daily caloric intake is derived directly from cereal grain consumption. Most of the grain used for human food is milled to remove the bran (pericarp) and germ, primarily to meet sensory expectations of consumers. The milling process strips the grains of important nutrients beneficial to health, including dietary fiber, phenolics, vitamins and minerals. Thus, even though ample evidence exists on the health benefits of whole grain consumption, challenges remain to developing food products that contain significant quantities of whole grain components and meet consumer expectations . This book presents some of the latest research endeavors that aim to improve our understanding of how the chemistry of various grain components can be manipulated to improve contribution of cereals to human health. Most of the topics are based on the Cereal Grains Symposium, at the 2011 American Chemical Society held in Anaheim, CA, March 27-31.},
  chapter = {1},
  isbn = {978-0-8412-2636-4},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\35BBMRDE\\Awika_2011_Major Cereal Grains Production and Use around the World.pdf;C\:\\Users\\daniel\\Zotero\\storage\\6LRSBVQD\\bk-2011-1089.html}
}

@article{fleurat-lessardAcousticDetectionAutomatic2006,
  title = {Acoustic {{Detection}} and {{Automatic Identification}} of {{Insect Stages Activity}} in {{Grain Bulks}} by {{Noise Spectra Processing Through Classification Algorithms}}.},
  author = {{Fleurat-Lessard}, F. and Tomasini, B. and Kostine, Laurent and Fuzeau, B.},
  year = {2006},
  urldate = {2023-08-05},
  abstract = {The activity of insects within a grain bulk produces noises in the audible range of wavelengths, which can be detected by high performance acoustic sensors. A portable probe of 1.4 m length was built up with three levels acoustical sensors coupled to a computer-assisted processing system. The recorded sound signals of the major grain insect species were digitized and stored into a reference database. A classification algorithm was developed for the automatic recognition of recorded insect noise signals by their comparison to the specific spectra of the reference database. The system was calibrated for Sitophilus oryzae and Rhyzopertha dominica , either at the adult or larval stage. The performances of the computer-assisted acoustic probe have been checked in small pilot scale conditions of 300-kg grain bin units. The distance to the sensor from which the insect noise spectrum was no more accurately identifiable was assessed at 20 cm, representing a ``sampled volume'' equivalent to 65 kg of wheat gain at each probing. For S. oryzae in wheat grain, the relationship between insect activity (either of larval or adult stage) and density levels was quantitatively modelled in the range from one individual per 10 kg to 10 individuals per kg at temperature levels from 5 to 30{$^\circ$}C. The cold stupor temperature of the rice weevil larva was assessed through the determination of the temperature level at the complete stop of activity deduced from the acoustical data. The threshold of temperature enabling insect larva activity was observed as low as 8{$^\circ$}C, i.e. much lower than was previously established, published, or believed. This new tool of early detection of an infestation in grain bulk will be now associated to decision support system for IPM implementation in grain handling and storing plants.},
  file = {C:\Users\daniel\Zotero\storage\SXNF3EC9\Fleurat-Lessard et al. - 2006 - Acoustic Detection and Automatic Identification of.pdf}
}

@article{thanushreeDetectionTechniquesInsect2018,
  title = {Detection {{Techniques}} for {{Insect Infestation}} in {{Stored Grains}}},
  author = {Thanushree, M. P. and Vimala, B. S. K. and Moses, J. A. and Anandharamakrishnan, C.},
  year = {2018},
  journal = {Agricultural Engineering Today},
  volume = {42},
  number = {4},
  pages = {48--56},
  issn = {2230-7265},
  urldate = {2023-08-05},
  abstract = {Food grain storage is an inevitable step in the grain ecosystem. During storage various abiotic and biotic factors interact with the food grains. Among those factors, the likelihood of pest infestation is very high. Infestation due to insects can not only cause quality and quantity loss in stored grains, it can also make the grain unfit for consumption. Storage structures are also affected due to insect infestation, thus contributing to huge loss. Hence, it is important to monitor insect infestation in stored grains. The objective of this work is to give an overview of the various detection techniques adopted to monitor pests of stored food grains. The principle of these techniques as well as their pros and cons are also explained to provide better understanding and aid in adopting the suitable technique.},
  langid = {english},
  keywords = {Detection,Insect infestation,Storage losses,Stored grains},
  file = {C:\Users\daniel\Zotero\storage\VZKU3IXR\Thanushree et al. - 2018 - Detection Techniques for Insect Infestation in Sto.pdf}
}

@inproceedings{sutinVibroacousticMethodsInsect2017,
  title = {Vibro-Acoustic Methods of Insect Detection in Agricultural Shipments and Wood Packing Materials},
  booktitle = {2017 {{IEEE International Symposium}} on {{Technologies}} for {{Homeland Security}} ({{HST}})},
  author = {Sutin, Alexander and Flynn, Timothy and Salloum, Hady and Sedunov, Nikolay and Sinelnikov, Yegor and {Hull-Sanders}, Helen},
  year = {2017},
  month = apr,
  pages = {1--6},
  doi = {10.1109/THS.2017.7943503},
  abstract = {Stevens Institute of Technology has been investigating engineering solutions to augment the current inspection process at ports of entry in an effort to minimize the threat posed by invasive species. Stevens has built several sensitive acoustic systems for detection of tiny acoustic/vibrational signals produced by moving insects and tested them in two US ports of entry and at the USDA APHIS PPQ S\&T facility in Buzzards Bay, MA. The system for insect detection in vegetables and herbs uses a soundproofed case where boxes filled with vegetables or herbs were placed during the test. For wood boring insects, sensitive custom-made accelerometers were built. Special algorithms for detection of events connected with insect movement and separating them from the ambient noise were developed. The tests demonstrating the system ability for insect detection were conducted with several representative insects, including: Khapra beetles in rice, Copitarsia larva and Helicoverpa zea in a large box of vegetables, and Asian Longhorned Beetles and Emerald Ash Borers in wood.},
  keywords = {Acoustics,Agriculture,Containers,Insects,Inspection,invasive species,Sensitivity,Vibrations,vibro-acoustic detection},
  file = {C:\Users\daniel\Zotero\storage\FGU9EFQS\Sutin et al. - 2017 - Vibro-acoustic methods of insect detection in agri.pdf}
}

@inproceedings{flynnAcousticMethodsInvasive2016,
  title = {Acoustic {{Methods}} of {{Invasive Species Detection}} in {{Agriculture Shipments}}},
  booktitle = {2016 {{IEEE Symposium}} on {{Technologies}} for {{Homeland Security}} ({{HST}})},
  author = {Flynn, Timothy and Salloum, Hady and {Hull-Sanders}, Helen and Sedunov, Alexander and Sedunov, Nikolay and Sinelnikov, Yegor and Sutin, Alexander and Masters, David},
  year = {2016},
  month = may,
  pages = {1--5},
  doi = {10.1109/THS.2016.7568897},
  abstract = {Stevens Institute of Technology in cooperation with the DHS Science and Technology Directorate and the U.S. Customs and Border Protection has been investigating engineering solutions to augment the current inspection process at ports of entry in an effort to minimize the threat posed by invasive species. This paper presents the initial results of acoustic sensors application detection of rodents and small insects in grains with the goal of improving the accuracy and effectiveness of current manual inspection methods.},
  keywords = {Acoustics,bioacoustics,cargo security,Containers,Insects,invasive species,Mice,Microphones,pest,rodents,Sensors,Vibrometers},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\56EMQCBQ\\Flynn et al. - 2016 - Acoustic methods of invasive species detection in .pdf;C\:\\Users\\daniel\\Zotero\\storage\\S2LBUCVM\\Flynn et al. - 2016 - Acoustic Methods of Invasive Species Detection in .pdf;C\:\\Users\\daniel\\Zotero\\storage\\SGREW798\\7568897.html}
}

@article{hagstrumEcologyIPMInsects2010,
  title = {Ecology and {{IPM}} of {{Insects}} at {{Grain Elevators}} and {{Flat Storages}}},
  author = {Hagstrum, D W and Flinn, P W and Reed, C R and Phillips, T W},
  year = {2010},
  volume = {6},
  number = {1},
  abstract = {Cost-effectiveness of insect pest management depends upon its integration with other elevator operations. Successful integration may require consideration of insect ecology. Field infestation has not been observed for grain received at elevators. Grain may be infested during harvest by residual insect infestations in the combines or may be infested soon after grain is loaded into bins by residual insect infestations at the elevator. A total of 61 species of insects in 26 families and 6 orders have been reported at elevators or in flat storages. At grain elevators, the species composition in grain residues differed from that in the wheat stored in bins. Insect populations in wheat increased from June to October, then leveled off and declined as fall and winter temperatures cooled the grain. Often less than 20\% of the bins had economically important insect infestations and generally the bins with high insect densities were close to other highly infested bins. The numbers of insects at elevators decreased with the depth below the surface as they did on farm. Insects are moved through the marketing systems with grain, but low sampling rates result in few of the infestations being discovered. Wheat with high-test weight was less likely to be discounted for insects than wheat with a lower test weight. Pest management practices were very effective for 20 out of 25 flat storages and infestations in others were not allowed to reach densities which would result in an infested designation on the grade certificate. Insect ecology is relevant to pest management in many ways. Segregating grain by moisture content and drying are important because low moisture grain is less susceptible to insects. Insects are highly mobile and grain residues inside and outside bins at an elevator that are not removed are a common source of the insects that infest other stored grain. Blending grain during unloading can minimize the amount of grain infested. Early summer aeration during the coldest hours of the night can slow insect population growth. Insect densities are highest near the surface where retention of the fumigant is most difficult. Knowing insect infestation levels can be important for profitable grain marketing.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\MGMPMCMU\Hagstrum et al. - 2010 - Ecology and IPM of Insects at Grain Elevators and .pdf}
}

@book{berleseApparecchioRaccoglierePresto1905,
  title = {{Apparecchio per raccogliere presto ed in gran numero piccoli arthropodi}},
  author = {Berlese, Antonio},
  year = {1905},
  googlebooks = {slIoAAAAYAAJ},
  langid = {italian}
}

@techreport{hoffmanStoredProductPestMonitoring2005,
  title = {Stored-{{Product Pest Monitoring Methods}}},
  author = {Hoffman, Eric R},
  year = {2005-May},
  institution = {Armed Forces Pest Management Board},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\AQ8DQMC3\Hoffman - 2005 - Stored-Product Pest Monitoring Methods.pdf}
}

@article{keszthelyiNonDestructiveImagingSpectroscopic2020,
  title = {Non-{{Destructive Imaging}} and {{Spectroscopic Techniques}} to {{Investigate}} the {{Hidden-Lifestyle Arthropod Pests}}: {{A Review}}},
  shorttitle = {Non-{{Destructive Imaging}} and {{Spectroscopic Techniques}} to {{Investigate}} the {{Hidden-Lifestyle Arthropod Pests}}},
  author = {Keszthelyi, S{\'a}ndor},
  year = {2020},
  month = jun,
  journal = {Journal of Plant Diseases and Protection},
  volume = {127},
  number = {3},
  pages = {283--295},
  issn = {1861-3837},
  doi = {10.1007/s41348-020-00300-6},
  urldate = {2023-10-23},
  abstract = {There are several harmful and yield decreasing arthropod pests, which live within plant tissues, causing almost unnoticeable damage, e.g. Ostrinia nubilalis Hbn., Cydia pomonella L., Acanthoscelides obtectus Say. Their ecological and biological features are rather known. The process leading to the damage is difficult to trace by means of conventional imaging techniques. In this review, optical techniques---X-ray, computer tomography, magnetic resonance imaging, confocal laser scanning microscopy, infrared thermography, near-infrared spectroscopy and luminescence spectroscopy---are described. Main results can contribute to the understanding of the covert pest life processes from the plant protection perspective. The use of these imaging technologies has greatly improved and facilitated the detailed investigation of injured plants. The results provided additional data on biological and ecological information as to the hidden lifestyles of covertly developing insects. Therefore, it can greatly contribute to the realisation of integrated pest management criteria in practical plant protection.},
  langid = {english},
  keywords = {3D imaging technology,Analysis,Arthropod pest,Hidden lifestyle,In vivo,Plant protection}
}

@misc{sonically_soundCrowdCafeteria2022,
  title = {Crowd at {{Cafeteria}}},
  author = {{sonically\_sound}},
  year = {2022},
  month = mar,
  urldate = {2023-10-28},
  abstract = {Freesound: collaborative database of creative-commons licensed sound for musicians and sound lovers. Have you freed your sound today?},
  howpublished = {https://freesound.org/people/sonically\_sound/sounds/625112/},
  langid = {english}
}

@misc{inspectorjAmbienceMachineFactory2017,
  title = {Ambience, {{Machine Factory}}, {{A}}},
  author = {{InspectorJ}},
  year = {2017},
  month = mar,
  urldate = {2023-10-28},
  abstract = {Freesound: collaborative database of creative-commons licensed sound for musicians and sound lovers. Have you freed your sound today?},
  howpublished = {https://freesound.org/people/InspectorJ/sounds/385943/},
  langid = {english}
}

@misc{viertelnachvierFactory22014,
  title = {Factory2},
  author = {{viertelnachvier}},
  year = {2014},
  month = sep,
  urldate = {2023-10-28},
  abstract = {Freesound: collaborative database of creative-commons licensed sound for musicians and sound lovers. Have you freed your sound today?},
  howpublished = {https://freesound.org/people/viertelnachvier/sounds/249637/},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\WHGZD5HD\249637.html}
}

@misc{XRayInspectionSystems2023,
  title = {X-{{Ray Inspection Systems}} - {{Product Purity}} by {{Sesotec}}},
  year = {2023},
  urldate = {2023-11-02},
  howpublished = {https://www.sesotec.com/na/en-US/products/x-ray-inspection-systems}
}

@misc{XRayFoodInspection2023,
  title = {X-{{Ray Food Inspection}} \& {{Detection Systems}}},
  year = {2023},
  urldate = {2023-11-02},
  abstract = {TDI PACKSYS offers the most precise X-ray inspection systems on the market. Our X-ray systems deliver best-in-class performance at a tremendous value.},
  howpublished = {https://www.tdipacksys.com/inspection-systems/x-ray/},
  langid = {american},
  file = {C:\Users\daniel\Zotero\storage\9KMXLS6L\x-ray.html}
}

@article{vigneronImmuneDefensesBeneficial2019,
  title = {Immune {{Defenses}} of a {{Beneficial Pest}}: {{The Mealworm Beetle}}, {{Tenebrio}} Molitor},
  shorttitle = {Immune {{Defenses}} of a {{Beneficial Pest}}},
  author = {Vigneron, Aur{\'e}lien and Jehan, Charly and Rigaud, Thierry and Moret, Yannick},
  year = {2019},
  journal = {Frontiers in Physiology},
  volume = {10},
  issn = {1664-042X},
  urldate = {2023-11-14},
  abstract = {The mealworm beetle, Tenebrio molitor, is currently considered as a pest when infesting stored grains or grain products. However, mealworms are now being promoted as a beneficial insect because their high nutrient content makes them a viable food source and because they are capable of degrading polystyrene and plastic waste. These attributes make T. molitor attractive for mass rearing, which may promote disease transmission within the insect colonies. Disease resistance is of paramount importance for both the control and the culture of mealworms, and several biotic and abiotic environmental factors affect the success of their anti-parasitic defenses, both positively and negatively. After providing a detailed description of T. molitor's anti-parasitic defenses, we review the main biotic and abiotic environmental factors that alter their presentation, and we discuss their implications for the purpose of controlling the development and health of this insect.}
}

@misc{XRayDetectionFood2023,
  title = {X-{{Ray Detection}} in the {{Food Industry}} - {{Overview}} by {{Sesotec}}},
  year = {2023},
  urldate = {2023-11-14},
  howpublished = {https://www.sesotec.com/na/en-US/resources/expertise/x-ray-inspection-systems-in-the-food-industry-quality-assurance},
  file = {C:\Users\daniel\Zotero\storage\LDBYTQXJ\x-ray-inspection-systems-in-the-food-industry-quality-assurance.html}
}

@article{thindDeterminationLowLevels2000,
  title = {Determination of {{Low Levels}} of {{Mite}} and {{Insect Contaminants}} in {{Food}} and {{Feedstuffs}} by a {{Modified Flotation Method}}},
  author = {Thind, B. B.},
  year = {2000},
  journal = {Journal of AOAC International},
  volume = {83},
  number = {1},
  pages = {113--119},
  issn = {1060-3271},
  abstract = {Extraneous material was separated from feed and food products by a modified technique in which kerosene is used in a specially designed flotation flask. This technique, although effective for analyzing feed and foods, presented limitations in the analysis of finely powdered materials. Some procedural modifications and an increased in the capacity of the flotation flask from 500 to 750 mL allowed a larger sample weight (20 g) to be analyzed for mites, insect fragments, and rodent hairs, with considerably reduced residue interference. In trials with a variety of products seeded with known numbers of mites, average recovery was 83\%. Recoveries of 89\% were obtained from flour samples seeded with insect fragments and rodent hairs. A new process of suspending extracted mites in a mixture of industrial methylated spirit (46\%) and glycerol (54\%) by volume was used to allow rapid and more precise estimates of mite populations in heavily infested samples.},
  langid = {english},
  pmid = {10693012},
  keywords = {Animal Feed,Animals,Food Analysis,Food Contamination,Hair,Indicators and Reagents,Insecta,Kerosene,Mites,Rodentia},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\HPMNG39L\\Thind - 2000 - Determination of Low Levels of Mite and Insect Con.pdf;C\:\\Users\\daniel\\Zotero\\storage\\GJH5GATB\\5580971.html}
}

@article{srivastavaFuzzyControllerBased2019,
  title = {Fuzzy {{Controller Based E-Nose Classification}} of {{Sitophilus Oryzae Infestation}} in {{Stored Rice Grain}}},
  author = {Srivastava, Shubhangi and Mishra, Gayatri and Mishra, Hari Niwas},
  year = {2019},
  month = jun,
  journal = {Food Chemistry},
  volume = {283},
  pages = {604--610},
  issn = {03088146},
  doi = {10.1016/j.foodchem.2019.01.076},
  urldate = {2023-11-15},
  abstract = {Fuzzy controller artmap based algorithms via E-nose selective metal oxides sensor (MOS) data was applied for classification of S. oryzae infestation in rice grains. The screened defuzzified data of selective sensors was further applied to detect S. oryzae infested rice with PCA and MLR techniques. Reliability of data was cross validated with reference methods of protein and uric acid content. Out of 18 MOS, 6 sensors namely P30/2, P30/1, T30/1, P40/2, T70/2 and PA/2 showed maximum resistivity change. Defuzzified score of 62.17 for P30/2 and 59.33 for P30/1 MOS further confirmed validity studies of E-nose sensor response with reference methods. The PCA plots were able to classify up to 84.75\% of rice with variable degree of S. oryzae infestation. The MLR values of predicted versus reference values of protein and uric acid content were found to be fitting with R2 of 0.972, 0.997 and RMSE values of 2.08, 1.05.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\CVZXXIVS\main.pdf}
}

@misc{ISO6639Cereals1986,
  title = {{{ISO}} 6639: {{Cereals}} and {{Pulses}} --- {{Determination}} of {{Hidden Insect Infestation}}},
  year = {1986},
  number = {6639},
  abstract = {The method consists in maintaining test samples at a controlled temperature and relative humidity such that the greatest possible proportion of the insects present in the sample when collected can develop to the adult stage, removing of insects that emerge from the grains, identification and counting, at close intervals, to enable the number initially present to be identified. An annex shows a suitable data record sheet.},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\5QP4MJSU\\ISO-6639-3-1986.pdf;C\:\\Users\\daniel\\Zotero\\storage\\Q8P96NWG\\et.iso.6639.4.2001.pdf}
}

@article{aliSustainableManagementInsect2023,
  title = {Towards {{Sustainable Management}} of {{Insect Pests}}: {{Protecting Food Security Through Ecological Intensification}}},
  author = {Ali, Mahmoud Abbas and Abdellah, Islam M and Eletmany, Mohamed R},
  year = {2023},
  abstract = {Insect pests are a major constraint to food security, destroying up to 40\% of global crop production annually. Pest outbreaks threaten smallholder incomes and disrupt supply, increasing market prices and reducing food accessibility for vulnerable populations. This review synthesizes evidence on strategies to minimize the negative economic impacts of insect pests and enhance the resilience of food systems. Recommended approaches include: adopting integrated pest management practices that balance multiple controls; transitioning to agroecological farming systems that boost biodiversity and ecological pest suppression; improving monitoring and rapid response to migratory pests; developing and disseminating resistant crop varieties; supporting farmer organizations and knowledge exchange networks; providing financial safety nets via insurance, credit and savings; investing in post-harvest storage and handling infrastructure; promoting livelihood diversification beyond crops; developing value-added processing for damaged crops; and raising consumer awareness to reduce waste and overconsumption. A mix of technological, organizational, and policy measures at multiple food system levels is required to reduce producer and consumer vulnerability to pest-induced supply and price shocks. Investment in ecologically based, socially-just approaches will strengthen food and economic security for those most at risk from the impacts of pest damage.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\YE8TSCA8\Ali et al. - 2023 - Towards Sustainable Management of Insect Pests Pr.pdf}
}

@article{eliopoulosEstimationPopulationDensity2016,
  title = {Estimation of {{Population Density}} of {{Stored Grain Pests Via Bioacoustic Detection}}},
  author = {Eliopoulos, Panagiotis A. and Potamitis, Ilyas and Kontodimas, Dimitris Ch.},
  year = {2016},
  month = jul,
  journal = {Crop Protection},
  volume = {85},
  pages = {71--78},
  issn = {0261-2194},
  doi = {10.1016/j.cropro.2016.04.001},
  urldate = {2023-12-24},
  abstract = {The potential of bioacoustics in estimating the population density of insect pests inside the stored grain mass was evaluated in the laboratory. We used a piezoelectric sensor and a portable acoustic emission amplifier connected to a computer for recording acoustic emissions of insects. The software analyses the vibration recordings of the piezoelectric sensor, performs signal parameterization and eventually classification of the infestation severity inside the grain mass in four classes, namely: Class A (densities {$\leq$}1 adult/kgr), Class B (densities 1--2 adults/kgr), Class C (densities 2--10 adults/kgr) and Class D (densities {$>$}10 adults/kgr). Adults of the most important beetle pests of stored cereals and pulses, in various population densities (1, 2, 10, 20, 50, 100, 200 \& 500 beetle adults/kgr grain) were used during the present study. The linear model was very effective in describing the relationship between population density and number of sounds. Multiple classifiers were used to evaluate the accuracy of bioacoustics on predicting the pest density given per minute counts of vibration pulses. Based on our results, our system's performance was very satisfactory in most cases ({$\sim$}68\%) given that probabilities for successful prediction typically exceeding 70\%. Our study suggests that automatic monitoring of infestations in bulk grain is feasible in small containers. This kind of service can assist with reliable decision making if it can be transferred to larger storage establishments (e.g. silos). Our results are discussed on the basis of enhancing the use of acoustic sensors as a decision support system in stored product IPM.},
  keywords = {Beetle pests,Bioacoustics,Classifiers,Insect,Population density,Stored grain},
  file = {C:\Users\daniel\Zotero\storage\WNNGSV6P\Eliopoulos et al. - 2016 - Estimation of population density of stored grain p.pdf}
}

@inproceedings{rabanoDeepTransferLearning2018,
  title = {Deep {{Transfer Learning}} Based {{Acoustic Detection}} of {{Rice Weevils}}, {{Sitophilus Oryzae}} ({{L}}.) in {{Stored Grains}}},
  author = {Rabano, Stephenn L. and Cabatuan, M. and Dadios, Elmer P. and Calilung, Edwin J. and Vicerra, R. R. and Sybingco, E. and Regpala, Efren R.},
  year = {2018},
  urldate = {2024-01-04},
  abstract = {The presence of rice weevils is causing degradation of rice quantity and quality during storage. Classifying rice grade is critical since rice weevils are not easily detected. This study used deep transfer learning on spectrogram images of sounds to recognize the presence or absence of rice weevils in a sound clip. There are 1000 audio files with rice weevil presence and 1000 audio files with the absence of rice weevils in the dataset, each having a duration of 5 seconds. Random environments and random number and age of insects were considered to have models that are less dependent on the environment setting. The dataset was preprocessed to generate the spectrogram image of each audio clip. Features of those images were extracted to train some pre-trained Keras models on the dataset. In the dataset, 1400 images were used for training and 600 were used for testing. Each among the models Xception, ResNet50, InceptionResNetV2, and MobileNet obtained a rank-1 accuracy of 99.17\% while VGG16, VGG19, and InceptionV3 all got a rank-1 accuracy of 99.33\%. The average precision, average recall, and average F1 score in each trained model are all 99\%. These account for the effectiveness of using deep transfer learning on spectrogram images of audio recordings in the detection of rice weevils in stored grains. This is also the first study that used deep transfer learning on spectrogram images in the acoustic detection of rice weevils.},
  file = {C:\Users\daniel\Zotero\storage\CCKF2CX8\Rabano et al. - 2018 - Deep Transfer Learning based Acoustic Detection of.pdf}
}

@article{muller-blenkleMethodAcousticStorage2023,
  title = {A {{Method}} for {{Acoustic Storage Pest Detection}} and {{Its Challenges}}},
  author = {{M{\"u}ller-Blenkle}, Christina and Simon, Ulrich and Meyer, Ralf and Szallies, Isabell and Lorenz, Daniela and Prozell, Sabine and Sch{\"o}ller, Matthias and Adler, Cornel S.},
  year = {2023},
  month = oct,
  journal = {Journal f{\"u}r Kulturpflanzen},
  volume = {75},
  number = {09-10},
  publisher = {Julius K{\"u}hn-Institut},
  issn = {1867-0911, 1867-0938},
  doi = {10.5073/JfK.2023.09-10.02},
  urldate = {2024-01-04},
  abstract = {Insects in grain can cause serious problems, not only because they feed on the grains. Mass reproduction also causes additional moisture and heat due to the...},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\5JHTZHBF\Müller-Blenkle et al. - 2023 - A Method for Acoustic Storage Pest Detection and I.pdf}
}

@article{devipriyar.InsectPestDetection2022,
  title = {Insect and {{Pest Detection}} in {{Stored Grains}}: {{Analysis}} of {{Environmental Factors}} and {{Comparison}} of {{Deep Learning Methods}}},
  shorttitle = {Insect and {{Pest Detection}} in {{Stored Grains}}},
  author = {{Devi Priya R.} and N., Anitha and V., Devisurya and V. P., Vidhyaa and K., Shobiya and C., Suguna},
  year = {2022},
  month = jun,
  journal = {WSEAS TRANSACTIONS ON ENVIRONMENT AND DEVELOPMENT},
  volume = {18},
  pages = {759--768},
  issn = {2224-3496, 1790-5079},
  doi = {10.37394/232015.2022.18.71},
  urldate = {2024-01-04},
  abstract = {Majority of the world's population depends on agro-based economy for their income and survival. In developing and under-developed countries, due to reasons like basic farming techniques, less educational and technological exposure, lack of technological advancements and recent agricultural knowledge, yield of the crops is very low and moreover there is a huge loss during storage also. Insects, pests and diseases more often affect the stored grains and cause heavy damage to the quantity and quality of the grains. Insecticides and pesticides cannot provide better solution all the times and hence there is an acute need for computer vision based techniques capable of monitoring the spread of insects in the initial stages of storage and protecting the stored grains from further damages and losses. Hence, this paper provides analysis of various factors which can cause damage to the stored grains natural ways to protect crops. It provides the comparison results of various standard deep learning methods that are used to detect the insects and pests in stored grains.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\Z3XXXJWH\R. et al. - 2022 - Insect and Pest Detection in Stored Grains Analys.pdf}
}

@article{hagstrumImprovingStoredProduct2019,
  title = {Improving {{Stored Product Insect Pest Management}}: {{From Theory}} to {{Practice}}},
  shorttitle = {Improving {{Stored Product Insect Pest Management}}},
  author = {Hagstrum, David W. and Athanassiou, Christos G.},
  year = {2019},
  month = oct,
  journal = {Insects},
  volume = {10},
  number = {10},
  pages = {332},
  issn = {2075-4450},
  doi = {10.3390/insects10100332},
  urldate = {2024-01-05},
  pmcid = {PMC6836213},
  pmid = {31590315},
  file = {C:\Users\daniel\Zotero\storage\4F26ZXFH\Hagstrum and Athanassiou - 2019 - Improving Stored Product Insect Pest Management F.pdf}
}

@article{srivastavaProbabilisticArtificialNeural2019,
  title = {Probabilistic {{Artificial Neural Network}} and {{E-Nose Based Classification}} of {{Rhyzopertha Dominica Infestation}} in {{Stored Rice Grains}}},
  author = {Srivastava, Shubhangi and Mishra, Gayatri and Mishra, Hari Niwas},
  year = {2019},
  month = mar,
  journal = {Chemometrics and Intelligent Laboratory Systems},
  volume = {186},
  pages = {12--22},
  issn = {01697439},
  doi = {10.1016/j.chemolab.2019.01.007},
  urldate = {2024-01-08},
  abstract = {The versatility of artificial neural network with back propagation multilayer perceptron approach could entitle an easy and methodical interpretation of results corresponding to multiple metal oxides sensor in an electronic nose. Three algorithms discriminant factorial analysis (DFA), soft independent modeling by class analogy (SIMCA), probabilistic artificial neural network (PANN) with back propagation multilayer perceptron (BPNN) were used for the classification of R. dominica infested rice stored for 225 days via 18 metal oxide sensors in E-nose. The coefficient of correlation for the three approaches were 88 (DFA), 96 (SIMCA), 98.96 (BPNN) \%, respectively. The percentage discrimination index was more distinct between 0 and 225 days R. dominica infested rice (98\%) than 0--180 days (93\%), and 0--135 days (88\%). The residual errors of validation and cross validation for SIMCA were 1.04 {\^A} 10{\`A}3 and 1.26 {\^A} 10{\`A}3 respectively. Major metal oxide sensors responsible for the production of volatiles were P30/1, T 30/1, PA/2, P30/2, T70/2, P40/1, and P40/2. The overall relative errors during artificial neural network training and testing were 0.092 and 0.286 respectively. The artificial neural network relative error for scale dependents in response to metal oxide sensors for mean, SD, \% RSD were 0.033, 0.162, 0.081, respectively. The applicability of E-nose with neural network could help in securing the data analysis time without loss of information and can also work well for noxious odors which might not be able to be categorized by human olfactory.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\VU8DMI6A\Srivastava et al. - 2019 - Probabilistic Artificial Neural Network and E-Nose.pdf}
}

@article{azizifarsaniBioacousticsTrogodermaGranarium2023,
  title = {Bioacoustics of {{Trogoderma Granarium Everts}} (Coleoptera: {{Dermestidae}})},
  shorttitle = {Bioacoustics of {{Trogoderma Granarium Everts}} (Coleoptera},
  author = {Azizi Farsani, Payam and Sakenian Dehkordi, Nader and Ebrahimi, Rahim and Nemati, Alireza and Taghizadeh Dehkordi, Maryam},
  year = {2023},
  month = dec,
  journal = {Journal of Asia-Pacific Entomology},
  pages = {102189},
  issn = {1226-8615},
  doi = {10.1016/j.aspen.2023.102189},
  urldate = {2024-01-08},
  abstract = {Trogoderma granarium Everts (Coleoptera: Dermestidae) is one of the major destructive pests of grain products in hot and dry parts of the world. No information has been made available on the bioacoustic signals (BASs) of T. granarium. In this paper, the bioacoustic features of this insect have measured from larvae and adult males and females. No anechoic chamber was used in this study. The extracted features were compared to evaluate the ability of the acoustic method to determine larva or adult as well as their sex. Adult males and females and larvae T. granarium produced BASs with a length of 2-11, 2-52, and 2-30 ms with a maximum sound pressure level of 3, 36, and 17 dB, respectively. At least 60\% of all BASs were in the ranges of 2-12 impulses in larva and adult stages. The sum of the total BASs was different in larvae and adult stages. Adult males had the least number of BASs compared to adult females and larvae. Detection of the growth stage and determination of the sex of the adult T. granarium by acoustic method in the time domain depends on the feature extracted from BASs and number of their impulses. Furthermore, in the frequency domain, the most dominant frequency bands were in the range of 1-8 kHz. In this band, the most dominant frequency band was 3.5-4.5 and 1-1.7 kHz for adult males and larvae, respectively. The most dominant bands of 1-3 and 4-5 kHz were observed for adult females.},
  keywords = {Acoustic method,Bioacoustic signals,Everts,Frequency domain,Impulse},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\58Y4XDRG\\Azizi Farsani et al. - 2023 - Bioacoustics of Trogoderma Granarium Everts (coleo.pdf;C\:\\Users\\daniel\\Zotero\\storage\\DY3TJAKM\\Azizi Farsani et al. - 2023 - Bioacoustics of Trogoderma Granarium Everts (coleo.pdf}
}

@article{gogiToxicityRepellencePlant2016,
  title = {Toxicity and {{Repellence}} of {{Plant Oils Against Tribolium Castaneum}} (Herbst), {{Rhyzopertha Dominica}} (f.) and {{Trogoderma Granarium}} (e.)},
  author = {Gogi, Muhammad Dildar and Hussain, Syed Makhdoom and Zia, Khuram},
  year = {2016},
  abstract = {Toxicity and repellency of Conocarpus erectus, Rosa indica and Cassia fistula oils extracted by Soxhlet apparatus in two solvents (petroleum ether and ethanol) were evaluated against Tribolium castaneum, Rhyzopertha dominica and Trogoderma granarium . The oils were applied on wheat grains at three concentrations (10, 20 and 30\%). The treated grains were offered to targeted insects. The toxicity results revealed that the ethanolic extracts were more effective against T. castaneum than petroleum-ether extracts. Ethanolic extracts of C. erectus proved more effective against T. castaneum followed by ethanolic extracts of C. fistula and R. indica. Petroleum ether extracts proved more effective against R. dominica and T. granarium because petroleum-ether extracts of C. erectus proved more effective followed by R. indica and C. fistula than their ethanolic extracts. The repellency data revealed that petroleum extracts proved more effective than ethanolic extracts against T. castaneum and R. dominica. Conocarpus erectus extracts were more repellent followed by C. fistula and R. indica However, C. erectus proved more effective repellent against T. granarium followed by R. indica and C. fistula. All the leaf-extracts investigated in present study can be better alternatives to synthetic grain protectants and should be further studied for characterization of the active molecules present in these extracts.},
  langid = {english},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\ABPQTMSQ\\Gogi et al. - 2016 - Toxicity and Repellence of Plant Oils Against Trib.pdf;C\:\\Users\\daniel\\Zotero\\storage\\DLGBN273\\20163318898.html}
}

@article{kumarReducingPostharvestLosses2017,
  title = {Reducing {{Postharvest Losses}} during {{Storage}} of {{Grain Crops}} to {{Strengthen Food Security}} in {{Developing Countries}}},
  author = {Kumar, Deepak and Kalita, Prasanta},
  year = {2017},
  month = jan,
  journal = {Foods},
  volume = {6},
  number = {1},
  pages = {8},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2304-8158},
  doi = {10.3390/foods6010008},
  urldate = {2024-01-08},
  abstract = {While fulfilling the food demand of an increasing population remains a major global concern, more than one-third of food is lost or wasted in postharvest operations. Reducing the postharvest losses, especially in developing countries, could be a sustainable solution to increase food availability, reduce pressure on natural resources, eliminate hunger and improve farmers' livelihoods. Cereal grains are the basis of staple food in most of the developing nations, and account for the maximum postharvest losses on a calorific basis among all agricultural commodities. As much as 50\%--60\% cereal grains can be lost during the storage stage due only to the lack of technical inefficiency. Use of scientific storage methods can reduce these losses to as low as 1\%--2\%. This paper provides a comprehensive literature review of the grain postharvest losses in developing countries, the status and causes of storage losses and discusses the technological interventions to reduce these losses. The basics of hermetic storage, various technology options, and their effectiveness on several crops in different localities are discussed in detail.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {food security,grain storage,hermetic storage,postharvest losses,smallholders},
  file = {C:\Users\daniel\Zotero\storage\AC7YFCV6\Kumar and Kalita - 2017 - Reducing Postharvest Losses during Storage of Grai.pdf}
}

@book{usdaGrainInspectionHandbook2004,
  title = {Grain {{Inspection Handbook}}},
  author = {{USDA}},
  year = {2004},
  publisher = {{US Department of Agriculture, Marketing and Regulatory Programs, Grain Inspection, Packers and Stockyards Administration, Federal Grain Inspection Service}},
  file = {C:\Users\daniel\Zotero\storage\6XTAXPTV\USDA - 2004 - Grain Inspection Handbook.pdf}
}

@inproceedings{elizabethb.maghirangDetectingSingleWheat2002,
  title = {Detecting {{Single Wheat Kernels Containing Live}} or {{Dead Insects Using Near-Infrared Reflectance Spectroscopy}}},
  booktitle = {2002 {{Chicago}}, {{IL July}} 28-31, 2002},
  author = {{Elizabeth B. Maghirang} and {Floyd E. Dowell} and {James E. Baker} and {James E. Throne}},
  year = {2002},
  publisher = {{American Society of Agricultural and Biological Engineers}},
  doi = {10.13031/2013.10449},
  urldate = {2024-01-08},
  abstract = {An automated near--infrared (NIR) reflectance system was used over a two--month storage period to detect single wheat kernels that contained live or dead internal rice weevils at various stages of growth. Correct classification of sound kernels plus kernels containing live pupae, large larvae, medium--sized larvae, and small larvae averaged 94\%, 93\%, 84\%, and 63\%, respectively. Pupae + large larvae calibrations were developed for live (day 1) and dead (days 7, 14, 28, 42, and 56) internal insects. Validation results showed that the live pupae +live large larvae calibration correctly classified 86\% to 96\% of dead pupae + dead large larvae validation samples. The dead pupae + dead large larvae calibration correctly detected the presence of live pupae + live large larvae with an accuracy of 92\% to 93\%. Thus, wheat kernels containing either live or dead insects can be used to develop calibrations for detecting both live and dead insects in wheat. These findings will impact how calibration sample sets can be handled. Results indicated that immediate sample processing for creating calibrations may no longer be necessary; internal insects can be killed and calibrations created at a later time without sacrificing accuracy. Additionally, laboratories can share these same calibration samples to save time and resources.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\II62HBLQ\Elizabeth B. Maghirang et al. - 2002 - Detecting Single Wheat Kernels Containing Live or .pdf}
}

@article{phillipsBiorationalApproachesManaging2010,
  title = {Biorational {{Approaches}} to {{Managing Stored-Product Insects}}},
  author = {Phillips, Thomas W. and Throne, James E.},
  year = {2010},
  month = jan,
  journal = {Annual Review of Entomology},
  volume = {55},
  number = {1},
  pages = {375--397},
  issn = {0066-4170, 1545-4487},
  doi = {10.1146/annurev.ento.54.110807.090451},
  urldate = {2024-01-10},
  abstract = {Stored-product insects can cause postharvest losses, estimated from up to 9\% in developed countries to 20\% or more in developing countries. There is much interest in alternatives to conventional insecticides for controlling stored-product insects because of insecticide loss due to regulatory action and insect resistance, and because of increasing consumer demand for product that is free of insects and insecticide residues. Sanitation is perhaps the first line of defense for grain stored at farms or elevators and for food-processing and warehouse facilities. Some of the most promising biorational management tools for farm-stored grain are temperature management and use of natural enemies. New tools for computer-assisted decision-making and insect sampling at grain elevators appear most promising. Processing facilities and warehouses usually rely on trap captures for decision-making, a process that needs further research to optimize.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\JJUGWX9K\1079_AnRE_55.375.pdf}
}

@techreport{mohamedn.sallamInsectDamagePostHarvest2013,
  title = {Insect {{Damage Post-Harvest Operations}}},
  author = {{Mohamed N. Sallam}},
  year = {2013},
  institution = {{International Centre of Insect Physiology and Ecology (ICIPE), Food and Agriculture Organization of the United Nations}},
  urldate = {2024-01-10},
  file = {C:\Users\daniel\Zotero\storage\IP8AFPCZ\Mohamed N. Sallam - 2013 - Insect Damage Post-Harvest Operations.pdf}
}

@misc{puiaudioPOM2735PRMicrophone2024,
  title = {{{POM-2735P-R Microphone}}},
  author = {{PUIAudio}},
  year = {2024},
  urldate = {2024-01-13},
  howpublished = {https://puiaudio.com/product/microphones/POM-2735P-R},
  langid = {american}
}

@article{pittendrighMonitoringRiceWeevil1997,
  title = {Monitoring of {{Rice Weevil}}, {{Sitophilus Oryzae}}, {{Feeding Behavior}} in {{Maize Seeds}} and the {{Occurence}} of {{Supernumerary Molts}} in {{Low Humidity Conditions}}},
  author = {Pittendrigh, B. R. and Huesing, J. E. and Shade, R. E. and Murdock, L. L.},
  year = {1997},
  journal = {Entomologia Experimentalis et Applicata},
  volume = {83},
  number = {2},
  pages = {225--231},
  issn = {0013-8703},
  doi = {10.1046/j.1570-7458.1997.00176.x},
  urldate = {2024-01-14},
  abstract = {Rice weevils, Sitophilus oryzae (L.), complete their development from egg to early adulthood inside grains of wheat, rice or maize, but little is known about their feeding behavior within the seeds. An ultrasonic insect feeding monitor was used to characterize the feeding patterns of individual rice weevils as they developed in maize grains. Weevils reared in grains held at ca. 40\% nh. took longer to develop from egg to pupation than those reared in grains kept at ca. 70\% nh. Feeding patterns revealed that larvae developing in grains held at ca. 70\% nh. had only four instars, whereas supernumerary molts (five instars) occurred in certain individuals reared in grain held at ca. 40\% nh. The supernumerary molts may represent a response of the insect to stresses associated with low moisture levels in the seeds.},
  keywords = {biomonitor,Biomonitor,feeding detection,Feeding detection,insect,Insect,insect feeding,Insect feeding,supernumerary molt,Supernumerary molt},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\FJXASVA8\\Pittendrigh et al. - 1997 - Monitoring of Rice Weevil, Sitophilus Oryzae, Feed.pdf;C\:\\Users\\daniel\\Zotero\\storage\\ZMQ9YL46\\Pittendrigh et al. - 1997 - Monitoring of Rice Weevil, Sitophilus Oryzae, Feed.pdf;C\:\\Users\\daniel\\Zotero\\storage\\KM78Y9QM\\j.1570-7458.1997.00176.html}
}

@article{j.c.webb.c.a.litzkowComputerizedAcousticalLarval1988,
  title = {A {{Computerized Acoustical Larval Detection System}}},
  author = {{J. C. Webb. C. A. Litzkow} and {D. C. Slaughter}},
  year = {1988},
  journal = {Applied Engineering in Agriculture},
  volume = {4},
  number = {3},
  pages = {268--274},
  issn = {1943-7838},
  doi = {10.13031/2013.26618},
  urldate = {2024-01-14},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\MM93HCVU\J. C. Webb. C. A. Litzkow and D. C. Slaughter - 1988 - A Computerized Acoustical Larval Detection System.pdf}
}

@article{vickAcousticLocationFixing1996,
  title = {Acoustic {{Location Fixing Insect Detector}}},
  author = {Vick, K. W.},
  year = {1996},
  journal = {The Journal of the Acoustical Society of America},
  volume = {99},
  number = {6},
  pages = {3277},
  issn = {00014966},
  doi = {10.1121/1.414906},
  urldate = {2024-01-15},
  abstract = {A device and a method for locating and counting insects in agricultural commodities has been developed. Acoustic sen Sors pick up Sounds emanating from adult and or larval insects. The electrical outputs of the sensors are amplified and analyzed to determine (1) the first sensor to detect the sound, (2) the second sensor to detect the sound and (3) the time difference between the first and second detection. These determinations are used to calculate the locations and num bers of insects in the agricultural commodity samples.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\ALXCB5BV\Vick - 1996 - Acoustic Location Fixing Insect Detector.pdf}
}

@article{hagstrumAutomatedAcousticalMonitoring1991,
  title = {Automated {{Acoustical Monitoring}} of {{Tribolium}} Castaneum ({{Coleoptera}}: {{Tenebrionidae}}) {{Populations}} in {{Stored Wheat}}},
  shorttitle = {Automated {{Acoustical Monitoring}} of {{Tribolium}} Castaneum ({{Coleoptera}}},
  author = {Hagstrum, David W. and Vick, Kenneth W. and Flinn, Paul W.},
  year = {1991},
  month = oct,
  journal = {Journal of Economic Entomology},
  volume = {84},
  number = {5},
  pages = {1604--1608},
  issn = {1938-291X, 0022-0493},
  doi = {10.1093/jee/84.5.1604},
  urldate = {2024-01-15},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\D95LR4X5\Hagstrum et al. - 1991 - Automated Acoustical Monitoring of Tribolium casta.pdf}
}

@article{shadeDetectionHiddenInsect1990,
  title = {Detection of {{Hidden Insect Infestations}} by {{Feeding-Generated Ultrasonic Signals}}},
  author = {Shade, Richard E. and Furgason, Eric S. and Murdock, Larry L.},
  year = {1990},
  journal = {American Entomologist},
  volume = {36},
  number = {3},
  pages = {231--235},
  issn = {2155-9902, 1046-2821},
  doi = {10.1093/ae/36.3.231},
  urldate = {2024-01-15},
  abstract = {Insects living hidden within seeds, wood, and certain other fibrous plant materials can be detected by the ultrasonic signals emitted when the insects feed on their organic substrate. Using cowpea weevil, Cal/osobruchus macuJatus (E), developing in cowpea seeds as our model, we have detected feeding activity by monitoring ultrasonic emissions at 40 KHz. The ultrasonic signals are produced when the insects strike the seed tissue during feeding, but not when they move in their tunnels. With cowpea weevil, feeding events can be detected from the early first ins tar through the last ins tar. The numbers of feeding events per unit time is directly proportional to the number of insects per seed. Many different species f hidden feeders can be detected by this technique, including other bruchids in a variety of legume seeds, SitophiJus spp. feeding in maize, lesser grain borer, Rhyzopertha dominica (E), feeding in rice, and termites feeding in wood. Ultrasonic monitoring should enable far greater insight into the life histories of insects feeding hidden within dry materials than has been possible by conventional techniques. Because the natural environment is relatively free of ultrasonic noise, monitoring insect activity is subject to little interference by extraneous signals. The "biomonitor," the electronic detector device developed to study the biology of feeding behavior, may have numerous practical applications, including early detection of hidden infestations, grain quality assessment, and rapid evaluation of efficacy of fumigation procedures.},
  langid = {english},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\EGHYCK5R\\Shade et al. - 1990 - Detection of Hidden Insect Infestations by Feeding.pdf;C\:\\Users\\daniel\\Zotero\\storage\\SZNW44VN\\2389204.html}
}

@article{claudiaNoiseSourcesCharacterization2012,
  title = {Noise {{Sources Characterization Inside}} and {{Outside}} a {{Factory}}},
  author = {Claudia, Tomozei and Astolfi, Arianna and Nedeff, V. and Lazar, Gabriel},
  year = {2012},
  month = mar,
  journal = {Environmental Engineering and Management Journal},
  volume = {11},
  pages = {701--708},
  doi = {10.30638/eemj.2012.089},
  abstract = {This work deals with noise pollution inside and outside a factory. Measurement of noise power level emitted by two sources located inside an industrial plant was carried out according to EN ISO 3746 and the transmission of noise coming from inside to outside was investigated by measurements and calculations according to EN 12354-4. SPL measurements were performed around the machines in a factory and the sound power level calculated after the influence of background noise and reverberation was excluded. The problems encountered in the application of the EN ISO 3746 measurement protocol and the calculation of the correction coefficient for undesired reflections was described. Noise was emitted from a source located inside the main building and SPL measurements were carried out near the noise source and outside the building at 3 m, 6 m, 12 m and 24 m. The measurement results were compared with the ones calculated, according to the EN 12354-4 Standard, showing a good correspondence. The maximum difference between measured and calculated data is 1.8 dB, at 3 m from the factory wall.},
  file = {C:\Users\daniel\Zotero\storage\GJ2NUAME\Claudia et al_2012_Noise sources characterization inside and outside a factory.pdf}
}

@article{sathiyaseelanMajorInsectPests2022,
  title = {Major {{Insect Pests}} of {{Oil Seed Crops}} and {{Their Management}}},
  author = {Sathiyaseelan, M and Balaji, K},
  year = {2022},
  month = dec,
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\ZXY7SA9S\Sathiyaseelan and Balaji - Major Insect Pests of Oil Seed Crops and Their Man.pdf}
}

@article{mankinMicrowaveRadarDetection2004,
  title = {Microwave {{Radar Detection}} of {{Stored-Product Insects}}},
  author = {Mankin, R. W.},
  year = {2004},
  month = jun,
  journal = {Journal of Economic Entomology},
  volume = {97},
  number = {3},
  pages = {1168--1173},
  issn = {0022-0493},
  doi = {10.1093/jee/97.3.1168},
  urldate = {2024-03-02},
  abstract = {A microwave radar system that senses motion was tested for capability to detect hidden insects of different sizes and activity levels in stored products. In initial studies, movements of individual adults or groups of Lasioderma serricorne (F.), Oryzaephilus surinamensis (L.), Attagenus unicolor (Brahm), and Tribolium castaneum (Herbst) were easily detected over distances up to 30 cm in air. Boxes of corn meal mix and flour mix were artificially infested with 5--100 insects to estimate the reliability of detection. The likelihood that a box was infested was rated by the radar system on a quantitative scale. The ratings were significantly correlated with the numbers of infesting insects. The radar system has potential applications in management programs where rapid, nondestructive targeting of incipient insect infestations would be of benefit to the producers and consumers of packaged foods.},
  file = {C:\Users\daniel\Zotero\storage\HP7IJB2G\Mankin - 2004 - Microwave Radar Detection of Stored-Product Insect.pdf}
}

@article{cabiSitophilusOryzaeLesser2021,
  title = {Sitophilus Oryzae (Lesser Grain Weevil)},
  author = {{CABI}},
  year = {2021},
  month = nov,
  journal = {CABI Compendium},
  volume = {CABI Compendium},
  pages = {10887},
  publisher = {CABI},
  doi = {10.1079/cabicompendium.10887},
  urldate = {2024-05-06},
  abstract = {This datasheet on Sitophilus oryzae covers Identity, Overview, Distribution, Hosts/Species Affected, Diagnosis, Biology \& Ecology, Natural Enemies, Impacts, Prevention/Control, Further Information.}
}

@article{gharaeeStepWorldwideBiodiversity2023,
  title = {A {{Step Towards Worldwide Biodiversity Assessment}}: {{The Bioscan-1m Insect Dataset}}},
  shorttitle = {A {{Step Towards Worldwide Biodiversity Assessment}}},
  author = {Gharaee, Zahra and Gong, ZeMing and Pellegrino, Nicholas and Zarubiieva, Iuliia and Haurum, Joakim Bruslund and Lowe, Scott and McKeown, Jaclyn and Ho, Chris and McLeod, Joschka and Wei, Yi-Yun and Agda, Jireh and Ratnasingham, Sujeevan and Steinke, Dirk and Chang, Angel and Taylor, Graham W. and Fieguth, Paul},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {43593--43619},
  urldate = {2024-05-21},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\73YQBCK8\Gharaee et al. - 2023 - A Step Towards Worldwide Biodiversity Assessment .pdf}
}

@article{tachibanaDefinitionMeasurementSound1987,
  title = {Definition and {{Measurement}} of {{Sound Energy Level}} of a {{Transient Sound Source}}},
  author = {Tachibana, Hideki and Yano, Hiroo and Yoshihisa, Koichi},
  year = {1987},
  journal = {Journal of the Acoustical Society of Japan (E)},
  volume = {8},
  number = {6},
  pages = {235--240},
  doi = {10.1250/ast.8.235},
  abstract = {Concerning stationary sound sources, sound power level which describes the sound power radiated by a sound source is clearly defined. For its measuring methods, the sound pressure methods using free field, hemi-free field and diffuse field have been established, and they have been standardized in the international and national standards. Further, the method of sound power measurement using the sound intensity technique has become popular. On the other hand, concerning transient sound sources such as impulsive and intermittent sound sources, the way of describing and measuring their acoustic outputs has not been established. In this paper, therefore, ``sound energy level'' which represents the total sound energy radiated by a single event of a transient sound source is first defined as contrasted with the sound power level. Subsequently, its measuring methods by two kinds of sound pressure method and sound intensity method are investigated theoretically and experimentally on referring to the methods of sound power level measurement.},
  file = {C:\Users\daniel\Zotero\storage\GC9IETTP\Tachibana et al. - 1987 - Definition and measurement of sound energy level o.pdf}
}

@misc{faissInsectSet32DatasetAutomatic2022,
  title = {{{InsectSet32}}: {{Dataset}} for Automatic Acoustic Identification of Insects ({{Orthoptera}} and {{Cicadidae}})},
  shorttitle = {{{InsectSet32}}},
  author = {Fai{\ss}, Marius},
  year = {2022},
  month = sep,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.7072196},
  urldate = {2024-05-29},
  abstract = {This dataset contains recordings of 32 sound producing insect species with~a total~335 files and a length of 57 minutes. The dataset was~compiled for training neural networks to automatically identify insect species while comparing adaptive, waveform-based frontends to conventional mel-spectrogram frontends for audio feature extraction. This work will be submitted for publication in the future and this dataset can be used to replicate the results, as well as other uses. A preprint of the paper is publicly available. The scripts for audio processing and the machine learning implementations~will be published on~Github. The recordings are split into two datasets.~Roughly half of the~recordings (147) are of nine species belonging to~the order Orthoptera. These recordings stem from a dataset that was~originally compiled by Baudewijn Od{\'e}~(unpublished).~ The remaining recordings~(188)~are of 23 species in the family Cicadidae.~These recordings were selected~from the Global Cicada Sound Collection hosted on~Bioacoustica~(doi.org/10.1093/database/bav054), including recordings published in~doi.org/10.3897/BDJ.3.e5792~\&~doi.org/10.11646/zootaxa.4340.1.~Many recordings from this collection included speech annotations in the beginning of the recordings, therefore the last ten seconds of audio were extracted and used in this dataset.~ All files were manually inspected and files with strong noise interference or with sounds of multiple species were removed. Between species, the number of files ranges from four to 22 files and the length from 40 seconds to almost nine minutes of audio material for a single species. The files range in length from less than one second to several minutes. All original files were available with sample rates of at least~44.1 kHz or higher but were resampled to 44.1 kHz mono WAV~files for consistency. The annotation files contain information for each recording, including the file name, species name and identifier, as well as the data subset they were included in for training the neural network (training, test, validation).},
  langid = {english},
  keywords = {bioacoustics,cicadidae,ecology,insecta,machine listening,orthoptera,remote monitoring},
  file = {C:\Users\daniel\Zotero\storage\XWRJR7BU\7072196.html}
}

@misc{faissInsectSet47InsectSet66Expanded2023,
  title = {{{InsectSet47}} \& {{InsectSet66}}: {{Expanded}} Datasets for Automatic Acoustic Identification of Insects ({{Orthoptera}} and {{Cicadidae}})},
  shorttitle = {{{InsectSet47}} \& {{InsectSet66}}},
  author = {Fai{\ss}, Marius},
  year = {2023},
  month = aug,
  publisher = {Zenodo},
  doi = {10.5281/zenodo.8252141},
  urldate = {2024-05-31},
  abstract = {Updated full version~with training, validation and test sets. Two newly compiled datasets for training neural networks to automatically identify insect species while comparing adaptive, waveform-based frontends to conventional mel-spectrogram frontends for audio feature extraction. This work was published~in PLOS Computational Biology and the machine learning implementations were published on Github. These datasets expand on the previously published InsectSet32 by including recently published collections of insect recordings by citizen scientists from around the world. Recordings from~BioAcoustica,~xeno-canto~and~iNaturalist, as well as private collections by~Baudewijn Od{\'e}~were downloaded and manually inspected. Files with strong noise interference or intense filtering, as well as files containing sounds of multiple species were removed to compile these datasets. The files were standardised to 44.1 kHz mono WAV files ranging in length from less than one second to several minutes. Files containing long periods without insect sounds were edited into multiple smaller files with silent periods no longer than 5 seconds. These files are marked as edits in the annotation file and should be assigned together into train/validation/test sets to prevent data leakage. The annotation files contain information for each recording, including the file name, species name and identifier, as well as the data subset they were included in for training the neural network (training, test, validation). InsectSet47 expands on InsectSet32 with recordings from~xeno-canto~and contains 1006 original recordings from 47 species, with at least ten files per species. The total length of InsectSet47 is 22 hours. InsectSet66 further expands on InsectSet47 by adding research-grade audio observations from~iNaturalist, with a total of 1554 recordings from 66 species, a total length of over 24 hours and a minimum of ten files per species. The datasets were split into the training, validation and test sets while ensuring a roughly equal distribution of audio files and audio material for every species in all three subsets. This resulted in a 60/20/20 split (train/validation/test) by file number and a 64/19.5/16.5 split by file length.},
  langid = {english},
  keywords = {bioacoustics,cicadidae,ecology,insecta,machine listening,orthoptera,remote monitoring},
  file = {C:\Users\daniel\Zotero\storage\9M5CWANM\8252141.html}
}

@misc{SingingInsectsNorth2024,
  title = {Singing {{Insects}} of {{North America}} ({{SINA}})},
  year = {2024},
  urldate = {2024-05-31}
}

@misc{danielkadyrovStoredProductInsect2024,
  title = {Stored {{Product Insect Dataset}} ({{SPID}}) - {{ASPIDS}}},
  author = {{Daniel Kadyrov} and {Daniel Zhao} and {Alexander Sutin} and {Sergey Samsonau} and {Alexander Sedunov} and {Nikolay Sedunov} and {Hady Salloum}},
  year = {2024},
  publisher = {Kaggle},
  doi = {10.34740/KAGGLE/DS/4982480},
  urldate = {2024-05-31}
}

@misc{danielkadyrovStoredProductInsect2024a,
  title = {Stored {{Product Insect Dataset}} ({{SPID}}) - {{MSPIDS}}},
  author = {{Daniel Kadyrov} and {Daniel Zhao} and {Alexander Sutin} and {Sergey Samsonau} and {Alexander Sedunov} and {Nikolay Sedunov} and {Hady Salloum}},
  year = {2024},
  publisher = {Kaggle},
  doi = {10.34740/KAGGLE/DSV/8381013},
  urldate = {2024-05-31}
}

@misc{AgriculturalPestsDataset2023,
  title = {Agricultural {{Pests Dataset}}},
  year = {2023},
  urldate = {2024-06-01},
  abstract = {Agricultural Pests Classification},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\JZUAKUF2\agricultural-pests-dataset.html}
}

@article{arslanRelationalDatabaseModel2017,
  title = {A {{Relational Database Model}} and {{Tools}} for {{Environmental Sound Recognition}}},
  author = {Arslan, Yuksel and Tan{\i}s, Abdussamet and Canbolat, Huseyin},
  year = {2017},
  month = dec,
  journal = {Advances in Science, Technology and Engineering Systems Journal},
  volume = {2},
  number = {6},
  pages = {145--150},
  issn = {24156698},
  doi = {10.25046/aj020618},
  urldate = {2024-06-07},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\BC9SBUQP\Arslan et al. - 2017 - A Relational Database Model and Tools for Environm.pdf}
}

@article{sawyerManagingSoundRelational,
  title = {Managing {{Sound}} in a {{Relational Multimedia Database System}}},
  author = {Sawyer, Gregory R},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\6XIG6A6M\Sawyer - Managing Sound in a Relational Multimedia Database.pdf}
}

@misc{NationalparkserviceIyoreEasing,
  title = {Nationalparkservice/Iyore: {{Easing}} the Thistly Problem of Accessing Data Stored in Arbitrary, Consistent Directory Structures},
  urldate = {2024-06-07},
  howpublished = {https://github.com/nationalparkservice/iyore},
  file = {C:\Users\daniel\Zotero\storage\I93E96R9\iyore.html}
}

@misc{Gjoseph92SoundDBQuery,
  title = {Gjoseph92/{{soundDB}}: {{Query}} and Load {{NSNSD}} Acoustic Data into {{Python}}, Minimize Pain},
  urldate = {2024-06-07},
  howpublished = {https://github.com/gjoseph92/soundDB?tab=readme-ov-file},
  file = {C:\Users\daniel\Zotero\storage\3Z4AFA8S\soundDB.html}
}

@inproceedings{hadadMultichannelAudioDatabase2014,
  title = {Multichannel Audio Database in Various Acoustic Environments},
  booktitle = {2014 14th {{International Workshop}} on {{Acoustic Signal Enhancement}} ({{IWAENC}})},
  author = {Hadad, Elior and Heese, Florian and Vary, Peter and Gannot, Sharon},
  year = {2014},
  month = sep,
  pages = {313--317},
  doi = {10.1109/IWAENC.2014.6954309},
  urldate = {2024-06-07},
  abstract = {In this paper we describe a new multichannel room impulse responses database. The impulse responses are measured in a room with configurable reverberation level resulting in three different acoustic scenarios with reverberation times RT60 equals to 160 ms, 360 ms and 610 ms. The measurements were carried out in recording sessions of several source positions on a spatial grid (angle range of -90{$^\circ$} to 90{$^\circ$} in 15{$^\circ$} steps with 1 m and 2 m distance from the microphone array). The signals in all sessions were captured by three microphone array configurations. The database is accompanied with software utilities to easily access and manipulate the data. Besides the description of the database we demonstrate its use in spatial source separation task.},
  keywords = {Acoustic measurements,Arrays,Database,Databases,microphone arrays,Microphones,multi-channel,Reverberation,room impulse response,Speech},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\WTD6RK5G\\Hadad et al. - 2014 - Multichannel audio database in various acoustic en.pdf;C\:\\Users\\daniel\\Zotero\\storage\\FSE2KLYK\\6954309.html}
}

@inproceedings{pattersonCUAVENewAudiovisual2002,
  title = {{{CUAVE}}: {{A}} New Audio-Visual Database for Multimodal Human-Computer Interface Research},
  shorttitle = {{{CUAVE}}},
  booktitle = {2002 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Patterson, E.K. and Gurbuz, S. and Tufekci, Z. and Gowdy, J.N.},
  year = {2002},
  month = may,
  volume = {2},
  pages = {II-2017-II-2020},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2002.5745028},
  urldate = {2024-06-07},
  abstract = {Multimodal signal processing has become an important topic of research for overcoming certain problems of audio-only speech processing. Audio-visual speech recognition is one area with great potential. Difficulties due to background noise and multiple speakers are significantly reduced by the additional information provided by extra visual features. Despite a few efforts to create databases in this area, none has emerged as a standard for comparison for several possible reasons. This paper seeks to introduce a new audiovisual database that is flexible and fairly comprehensive, yet easily available to researchers on one DVD. The CUAVE database is a speaker-independent corpus of over 7,000 utterances of both connected and isolated digits. It is designed to meet several goals that are discussed in this paper. The most notable are availability of the database, flexibility for use of the audio-visual data, and realistic considerations in the recordings (such as speaker movement). Another important focus of the database is the inclusion of pairs of simultaneous speakers, the first documented database of this kind. The overall goal of this project is to facilitate more widespread audio-visual research through an easily available database. For information on obtaining CUAVE, please visit our webpage (http://ece.clemson.edu/speech).},
  keywords = {Databases,Ear,Visualization},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\D9PQJHN2\\Patterson et al. - 2002 - CUAVE A new audio-visual database for multimodal .pdf;C\:\\Users\\daniel\\Zotero\\storage\\8VC4GI3H\\5745028.html}
}

@article{seshadriEnhancedAbstractData1998,
  title = {Enhanced Abstract Data Types in Object-Relational Databases},
  author = {Seshadri, Praveen},
  year = {1998},
  month = aug,
  journal = {The VLDB Journal},
  volume = {7},
  number = {3},
  pages = {130--140},
  issn = {0949-877X},
  doi = {10.1007/s007780050059},
  urldate = {2024-06-07},
  abstract = {The explosion in complex multimedia content makes it crucial for database systems to support such data efficiently. This paper argues that the ``blackbox'' ADTs used in current object-relational systems inhibit their performance, thereby limiting their use in emerging applications. Instead, the next generation of object-relational database systems should be based on enhanced abstract data type (E-ADT) technology. An (E-ADT) can expose the semantics of its methods to the database system, thereby permitting advanced query optimizations. Fundamental architectural changes are required to build a database system with E-ADTs; the added functionality should not compromise the modularity of data types and the extensibility of the type system. The implementation issues have been explored through the development of E-ADTs in Predator. Initial performance results demonstrate an order of magnitude in performance improvements.},
  langid = {english},
  keywords = {Key words:Object-relational database - Extensibility - Database types - Query optimization},
  file = {C:\Users\daniel\Zotero\storage\AZRT2MII\Seshadri - 1998 - Enhanced abstract data types in object-relational .pdf}
}

@article{thiemannDiverseEnvironmentsMultichannel2013,
  title = {The {{Diverse Environments Multi-channel Acoustic Noise Database}} ({{DEMAND}}): {{A}} Database of Multichannel Environmental Noise Recordings},
  shorttitle = {The {{Diverse Environments Multi-channel Acoustic Noise Database}} ({{DEMAND}})},
  author = {Thiemann, Joachim and Ito, Nobutaka and Vincent, Emmanuel},
  year = {2013},
  month = may,
  journal = {Proceedings of Meetings on Acoustics},
  volume = {19},
  number = {1},
  pages = {035081},
  issn = {1939-800X},
  doi = {10.1121/1.4799597},
  urldate = {2024-06-07},
  abstract = {Multi-microphone arrays allow for the use of spatial filtering techniques that can greatly improve noise reduction and source separation. However, for speech and audio data, work on noise reduction or separation has focused primarily on one- or two-channel systems. Because of this, databases of multichannel environmental noise are not widely available. DEMAND (Diverse Environments Multi-channel Acoustic Noise Database) addresses this problem by providing a set of 16-channel noise files recorded in a variety of indoor and outdoor settings. The data was recorded using a planar microphone array consisting of four staggered rows, with the smallest distance between microphones being 5 cm and the largest being 21.8 cm. DEMAND is freely available under a Creative Commons license to encourage research into algorithms beyond the stereo setup.},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\8LKY4ZQ5\\Thiemann et al. - 2013 - The Diverse Environments Multi-channel Acoustic No.pdf;C\:\\Users\\daniel\\Zotero\\storage\\KMVC95N5\\The-Diverse-Environments-Multi-channel-Acoustic.html}
}

@article{mckayLargePubliclyAccessible,
  title = {A {{Large Publicly Accessible Prototype Audio Database}} for {{Music Research}}},
  author = {McKay, Cory and McEnnis, Daniel and Fujinaga, Ichiro},
  abstract = {This paper introduces Codaich, a large and diverse publicly accessible database of musical recordings for use in music information retrieval (MIR) research. The issues that must be dealt with when constructing such a database are discussed, as are ways of addressing these problems. It is suggested that copyright restrictions may be overcome by allowing users to make customized feature extraction queries rather than allowing direct access to recordings themselves. The jMusicMetaManager software is introduced as a tool for improving metadata associated with recordings by automatically detecting inconsistencies and redundancies.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\HP65P6FX\McKay et al. - A Large Publicly Accessible Prototype Audio Databa.pdf}
}

@article{liCHEAVDChineseNatural2017,
  title = {{{CHEAVD}}: A {{Chinese}} Natural Emotional Audio--Visual Database},
  shorttitle = {{{CHEAVD}}},
  author = {Li, Ya and Tao, Jianhua and Chao, Linlin and Bao, Wei and Liu, Yazhu},
  year = {2017},
  month = nov,
  journal = {Journal of Ambient Intelligence and Humanized Computing},
  volume = {8},
  number = {6},
  pages = {913--924},
  issn = {1868-5145},
  doi = {10.1007/s12652-016-0406-z},
  urldate = {2024-06-07},
  abstract = {This paper presents a recently collected natural, multimodal, rich-annotated emotion database, CASIA Chinese Natural Emotional Audio--Visual Database (CHEAVD), which aims to provide a basic resource for the research on multimodal multimedia interaction. This corpus contains 140~min emotional segments extracted from films, TV plays and talk shows. 238 speakers, aging from child to elderly, constitute broad coverage of speaker diversity, which makes this database a valuable addition to the existing emotional databases. In total, 26 non-prototypical emotional states, including the basic six, are labeled by four native speakers. In contrast to other existing emotional databases, we provide multi-emotion labels and fake/suppressed emotion labels. To our best knowledge, this database is the first large-scale Chinese natural emotion corpus dealing with multimodal and natural emotion, and free to research use. Automatic emotion recognition with Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) is performed on this corpus. Experiments show that an average accuracy of 56~\% could be achieved on six major emotion states.},
  langid = {english},
  keywords = {Audio-visual database,Corpus annotation,LSTM,Multimodal emotion recognition,Natural emotion},
  file = {C:\Users\daniel\Zotero\storage\HPEACTYH\Li et al. - 2017 - CHEAVD a Chinese natural emotional audio–visual d.pdf}
}

@inproceedings{sturimSpeakerIndexingLarge2001,
  title = {Speaker Indexing in Large Audio Databases Using Anchor Models},
  booktitle = {2001 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}} ({{Cat}}. {{No}}.{{01CH37221}})},
  author = {Sturim, D.E. and Reynolds, D.A. and Singer, E. and Campbell, J.P.},
  year = {2001},
  month = may,
  volume = {1},
  pages = {429-432 vol.1},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2001.940859},
  urldate = {2024-06-07},
  abstract = {Introduces the technique of anchor modeling in the applications of speaker detection and speaker indexing. The anchor modeling algorithm is refined by pruning the number of models needed. The system is applied to the speaker detection problem where its performance is shown to fall short of the state-of-the-art Gaussian mixture model with universal background model (GMM-UBM) system. However, it is further shown that its computational efficiency lends itself to speaker indexing for searching large audio databases for desired speakers. Here, excessive computation may prohibit the use of the GMM-UBM recognition system. Finally, the paper presents a method for cascading anchor model and GMM-UBM detectors for speaker indexing. This approach benefits from the efficiency of anchor modeling and high accuracy of GMM-UBM recognition.},
  keywords = {Audio databases,Computational efficiency,Contracts,Detectors,Embedded computing,Hidden Markov models,Indexing,Laboratories,Speaker recognition,Speech},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\CLBCMAL3\\Sturim et al. - 2001 - Speaker indexing in large audio databases using an.pdf;C\:\\Users\\daniel\\Zotero\\storage\\ZQWAJQ2Y\\940859.html}
}

@inproceedings{foxVALIDNewPractical2005,
  title = {{{VALID}}: {{A New Practical Audio-Visual Database}}, and {{Comparative Results}}},
  shorttitle = {{{VALID}}},
  booktitle = {Audio- and {{Video-Based Biometric Person Authentication}}},
  author = {Fox, Niall A. and O'Mullane, Brian A. and Reilly, Richard B.},
  editor = {Kanade, Takeo and Jain, Anil and Ratha, Nalini K.},
  year = {2005},
  pages = {777--786},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11527923_81},
  abstract = {The performance of deployed audio, face, and multi-modal person recognition systems in non-controlled scenarios, is typically lower than systems developed in highly controlled environments. With the aim to facilitate the development of robust audio, face, and multi-modal person recognition systems, the new large and realistic multi-modal (audio-visual) VALID database was acquired in a noisy ``real world'' office scenario with no control on illumination or acoustic noise. In this paper we describe the acquisition and content of the VALID database, consisting of five recording sessions of 106 subjects over a period of one month. Speaker identification experiments using visual speech features extracted from the mouth region are reported. The performance based on the uncontrolled VALID database is compared with that of the controlled XM2VTS database. The best VALID and XM2VTS based accuracies are 63.21\% and 97.17\% respectively. This highlights the degrading effect of an uncontrolled illumination environment and the importance of this database for deploying real world applications. The VALID database is available to the academic community through http://ee.ucd.ie/validdb/.},
  isbn = {978-3-540-31638-1},
  langid = {english},
  keywords = {Discrete Cosine Transform,Hide Markov Model,Speaker Identification,Speaker Recognition,Visual Speech},
  file = {C:\Users\daniel\Zotero\storage\LEUD8K5M\Fox et al. - 2005 - VALID A New Practical Audio-Visual Database, and .pdf}
}

@inproceedings{magrin-chagnolleauDetectionTargetSpeakers1999,
  title = {Detection of Target Speakers in Audio Databases},
  booktitle = {1999 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}. {{Proceedings}}. {{ICASSP99}} ({{Cat}}. {{No}}.{{99CH36258}})},
  author = {{Magrin-Chagnolleau}, I. and Rosenberg, A.E. and Parthasarathy, S.},
  year = {1999},
  month = mar,
  volume = {2},
  pages = {821-824 vol.2},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.1999.759797},
  urldate = {2024-06-07},
  abstract = {The problem of speaker detection in audio databases is addressed in this paper. Gaussian mixture modeling is used to build target speaker and background models. A detection algorithm based on a likelihood ratio calculation is applied to estimate target speaker segments. Evaluation procedures are defined in detail for this task. Results are given for different subsets of the HUB4 broadcast news database. For one target speaker, with the data restricted to high quality speech segments, the segment miss rate is approximately 7\%. For unrestricted data, the segment miss rate is approximately 27\%. In both cases the segment false alarm rate is 4 or 5 per hour. For two target speakers with unrestricted data, the segment miss rate is approximately 63\% with about 27 segment false alarms per hour. The decrease in performance for two target speakers is largely associated with short speech segments in the two target speaker test data which are undetectable in the current configuration of the detection algorithm.},
  keywords = {Audio databases,Broadcasting,Concatenated codes,Detection algorithms,Speech,Testing},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\J8ANTASB\\Magrin-Chagnolleau et al. - 1999 - Detection of target speakers in audio databases.pdf;C\:\\Users\\daniel\\Zotero\\storage\\88872NLY\\759797.html}
}

@article{svanstromDroneDetectionTracking2022,
  title = {Drone {{Detection}} and {{Tracking}} in {{Real-Time}} by {{Fusion}} of {{Different Sensing Modalities}}},
  author = {Svanstr{\"o}m, Fredrik and {Alonso-Fernandez}, Fernando and Englund, Cristofer},
  year = {2022},
  month = nov,
  journal = {Drones},
  volume = {6},
  number = {11},
  pages = {317},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2504-446X},
  doi = {10.3390/drones6110317},
  urldate = {2024-06-07},
  abstract = {Automatic detection of flying drones is a key issue where its presence, especially if unauthorized, can create risky situations or compromise security. Here, we design and evaluate a multi-sensor drone detection system. In conjunction with standard video cameras and microphone sensors, we explore the use of thermal infrared cameras, pointed out as a feasible and promising solution that is scarcely addressed in the related literature. Our solution integrates a fish-eye camera as well to monitor a wider part of the sky and steer the other cameras towards objects of interest. The sensing solutions are complemented with an ADS-B receiver, a GPS receiver, and a radar module. However, our final deployment has not included the latter due to its limited detection range. The thermal camera is shown to be a feasible solution as good as the video camera, even if the camera employed here has a lower resolution. Two other novelties of our work are the creation of a new public dataset of multi-sensor annotated data that expands the number of classes compared to existing ones, as well as the study of the detector performance as a function of the sensor-to-target distance. Sensor fusion is also explored, showing that the system can be made more robust in this way, mitigating false detections of the individual sensors.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {anti-drone systems,drone detection,UAV detection},
  file = {C:\Users\daniel\Zotero\storage\9TK4GJ3I\Svanström et al. - 2022 - Drone Detection and Tracking in Real-Time by Fusio.pdf}
}

@article{tsanousaReviewMultisensorData2022,
  title = {A {{Review}} of {{Multisensor Data Fusion Solutions}} in {{Smart Manufacturing}}: {{Systems}} and {{Trends}}},
  shorttitle = {A {{Review}} of {{Multisensor Data Fusion Solutions}} in {{Smart Manufacturing}}},
  author = {Tsanousa, Athina 1 and Bektsis, Evangelos 1 and Kyriakopoulos, Constantine 1 and Gonz{\'a}lez~2, Ana G{\'o}mez and Leturiondo, Urko 2 and Gialampoukidis, Ilias 1 and Karakostas, Anastasios 1 and Vrochidis, Stefanos 1 and Kompatsiaris, Ioannis 1 1 Information Technologies Institute and Technology Hellas, 6th km Charilaou-Thermi Road and {evanbekt@iti.gr (E.B.)} and {kyriak@iti.gr (C.K.)} and {heliasgj@iti.gr (I.G.)} and {akarakos@iti.gr (A.K.)} and {stefanos@iti.gr (S.V.)} and {ikom@iti. gr (I.K.)~2~~Ikerlan Technology Research Centre}, Basque Research and Technology Alliance (BRTA), Po J. Ma Arizmendiarrieta 2 and {ana.gomez@ikerlan.es (A.G.G.)} and {uleturiondo@ikerlan.es (U.L.)}},
  year = {2022},
  pages = {1734},
  publisher = {MDPI AG},
  doi = {10.3390/s22051734},
  urldate = {2024-06-07},
  abstract = {Manufacturing companies increasingly become ``smarter'' as a result of the Industry 4.0 revolution. Multiple sensors are used for industrial monitoring of machines and workers in order to detect events and consequently improve the manufacturing processes, lower the respective costs, and increase safety. Multisensor systems produce big amounts of heterogeneous data. Data fusion techniques address the issue of multimodality by combining data from different sources and improving the results of monitoring systems. The current paper presents a detailed review of state-of-the-art data fusion solutions, on data storage and indexing from various types of sensors, feature engineering, and multimodal data integration. The review aims to serve as a guide for the early stages of an analytic pipeline of manufacturing prognosis. The reviewed literature showed that in fusion and in preprocessing, the methods chosen to be applied in this sector are beyond the state-of-the-art. Existing weaknesses and gaps that lead to future research goals were also identified.},
  copyright = {{\copyright} 2022 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\LQV8C4E5\Tsanousa et al. - 2022 - A Review of Multisensor Data Fusion Solutions in S.pdf}
}

@inproceedings{sedunovStevensPassiveAcoustic2022a,
  title = {Stevens {{Passive Acoustic Detection System}} ({{SPADES-2}}) and Its Prospective Application for Windfarm Underwater Noise Assessment},
  booktitle = {182nd {{Meeting}} of the {{Acoustical Society}} of {{America}}},
  author = {Sedunov, Alexander and Salloum, Hady and Sedunov, Nikolay and Francis, Christopher and Tsyuryupa, Sergey and Merzhevskiy, Aleksandr and Kadyrov, Daniel and Sutin, Alexander},
  year = {2022},
  pages = {070001},
  address = {Denver, Colorado},
  doi = {10.1121/2.0001626},
  urldate = {2024-06-07},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\G8K2SV26\Sedunov et al. - 2022 - Stevens Passive Acoustic Detection System (SPADES-.pdf}
}

@misc{soerenabSoerenabAudioMNIST2024,
  title = {Soerenab/{{AudioMNIST}}},
  author = {{soerenab}},
  year = {2024},
  month = may,
  urldate = {2024-06-08},
  copyright = {MIT}
}

@misc{FacebookresearchRealacousticfields2024,
  title = {Facebookresearch/Real-Acoustic-Fields},
  year = {2024},
  month = jun,
  urldate = {2024-06-08},
  abstract = {Real Acoustic Fields An Audio-Visual Room Acoustics Dataset and Benchmark},
  howpublished = {Meta Research}
}

@misc{ArabicSpeechCorpus,
  title = {Arabic {{Speech Corpus}}},
  urldate = {2024-06-08},
  howpublished = {https://en.arabicspeechcorpus.com/},
  file = {C:\Users\daniel\Zotero\storage\KGBP9M95\en.arabicspeechcorpus.com.html}
}

@misc{ActedEmotionalSpeech,
  title = {Acted {{Emotional Speech Dynamic Database}} - {{AESDD}}},
  journal = {M3C},
  urldate = {2024-06-08},
  abstract = {Acted Emotional Speech Dynamic Database (AESDD) is a speech emotion recognition dataset in Greek . You can download it for experiments or contribute to it.},
  howpublished = {https://m3c.web.auth.gr/research/aesdd-speech-emotion-recognition/},
  langid = {american},
  file = {C:\Users\daniel\Zotero\storage\L6XUDWYL\aesdd-speech-emotion-recognition.html}
}

@misc{CheyneyComputerScienceCREMAD2024,
  title = {{{CheyneyComputerScience}}/{{CREMA-D}}},
  year = {2024},
  month = jun,
  urldate = {2024-06-08},
  abstract = {Crowd Sourced Emotional Multimodal Actors Dataset (CREMA-D)},
  howpublished = {Cheyney Computer Science}
}

@article{kadyrovImprovementsStevensDrone2022,
  title = {Improvements to the {{Stevens}} Drone Acoustic Detection System},
  author = {Kadyrov, Daniel and Sedunov, Alexander and Sedunov, Nikolay and Sutin, Alexander and Salloum, Hady and Tsyuryupa, Sergey},
  year = {2022},
  month = aug,
  journal = {Proceedings of Meetings on Acoustics},
  volume = {46},
  number = {1},
  pages = {045001},
  issn = {1939-800X},
  doi = {10.1121/2.0001602},
  urldate = {2024-06-08},
  abstract = {Stevens Institute of Technology (SIT) designed and built multiple acoustic sensors to detect and track drones using Steered-Response Phase Transform (SRP-PHAT) and classify them using narrow-band frequencies. SIT improved a previously built four-microphone system by increasing the number of microphones to seven, modifying the software, and improving the testing algorithm for system performance estimation. The four- and the seven-microphone systems were deployed during several tests conducted with multi-rotor sUAS of different sizes, including the DJI Inspire 2, DJI Mavic 2 Pro, Autel Robotics EVO II Pro, and the Intel Falcon 8 performing flight patterns at various distances, elevations, and speeds. Acoustic signatures were collected and detection distances were compared for the tested drones. Measurements from the field test were used to recalculate UAS acoustic signatures to 1 meter. These signatures were applied to develop a simple method of acoustic detection distance estimation using the passive sonar equation. The improved seven-microphone system provided farther detection distances than the four-microphone system.},
  copyright = {All rights reserved},
  file = {C:\Users\daniel\Zotero\storage\2GPI559T\Kadyrov et al. - 2022 - Improvements to the Stevens drone acoustic detecti.pdf}
}

@article{caseyAcousticLexemesOrganizing2005,
  title = {Acoustic Lexemes for Organizing Internet Audio},
  author = {Casey, Michael A.},
  year = {2005},
  month = dec,
  journal = {Contemporary Music Review},
  volume = {24},
  number = {6},
  pages = {489--508},
  issn = {0749-4467, 1477-2256},
  doi = {10.1080/07494460500296169},
  urldate = {2024-06-11},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\QQKMCVZ5\Casey - 2005 - Acoustic lexemes for organizing internet audio.pdf}
}

@inproceedings{huObjectrelationalDatabaseSystem1997,
  title = {An Object-Relational Database System for the Interactive Multimedia},
  booktitle = {1997 {{IEEE International Conference}} on {{Intelligent Processing Systems}} ({{Cat}}. {{No}}.{{97TH8335}})},
  author = {Hu, M.J. and Chunyan, Miao},
  year = {1997},
  month = oct,
  volume = {2},
  pages = {1571-1575 vol.2},
  doi = {10.1109/ICIPS.1997.669298},
  urldate = {2024-06-11},
  abstract = {A practical and effective data model for interactive multimedia is provided in the authors' project. It is shown that the data model is not only adequate to represent various types of media entities including audio, video, image and text, but also competent for facilitating hypermedia relationships among different entities. Based on such a data model, they have designed and developed an object-relational multimedia database infrastructure, with a number of useful schemas for supporting multimedia services. Apart from standard authoring, editing, and searching/querying features, advanced options such as hypermedia navigating and browsing are also supported in the object-relational multimedia database (ORDB). A prototype of the proposed ORDB is being implemented for interactive multimedia applications.},
  keywords = {Data models,Database systems,Multimedia databases,Multimedia systems,Navigation,Prototypes},
  file = {C:\Users\daniel\Zotero\storage\V9CTWKW4\669298.html}
}

@book{sakrGraphDataManagement2011,
  title = {Graph {{Data Management}}: {{Techniques}} and {{Applications}}},
  shorttitle = {Graph {{Data Management}}},
  author = {Sakr, Sherif and Pardede, Eric},
  year = {2011},
  month = jan,
  doi = {10.4018/978-1-61350-053-8},
  isbn = {978-1-61350-053-8},
  file = {C:\Users\daniel\Zotero\storage\PR326VWG\Sakr and Pardede - 2011 - Graph Data Management Techniques and Applications.pdf}
}

@article{varlamisUSINGXMLMEDIUM,
  title = {{{USING XML AS A MEDIUM FOR DESCRIBING}}, {{MODIFYING AND QUERYING AUDIOVISUAL CONTENT STORED IN RELATIONAL DATABASE SYSTEMS}}},
  author = {Varlamis, Iraklis and Vazirgiannis, Michalis and Poulos, Panagiotis},
  abstract = {The digitization and annotation of audiovisual programs results in a huge amount of information that becomes useful only if it is properly organized and if the appropriate query mechanisms exist. Usually audiovisual information and meta-information is stored in relational database schemata that consist of decades of tables, relationships and constraints that complicate the information querying tasks. The PANORAMA* platform has been developed to cover needs for manipulating video information, by attaching meta-information for both audio and visual content of video sources. This paper presents this meta-information model and the database interface developed in terms of the PANORAMA platform. The model works as keystone in the creation of the database and the database interface implements a mechanism for converting XML documents to relational data. This mechanism allows information users to use a more understandable way to communicate with the database.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\W2FN8D5S\Varlamis et al. - USING XML AS A MEDIUM FOR DESCRIBING, MODIFYING AN.pdf}
}

@article{ohmanAudiovisualSpeechDatabase,
  title = {An Audio-Visual Speech Database and Automatic Measurements of Visual Speech},
  author = {{\"O}hman, T},
  abstract = {A database of video sequences of a male speaker uttering 269 Swedish sentences and 153 VCV-words has been recorded. Parts of the speaker's face were marked to facilitate optical measurements. Algorithms to automatically determine the shape of the lips as well as areas and other visual features of the speaker's face have been developed. The audio signal has been phonetically segmented and labelled. This material differs from most other audio-visual databases mainly in two aspects: firstly the recorded material contains naturally reduced and coarticulated continuously spoken sentences, and secondly, a specially constructed device provides the possibility to easily trace jaw movements.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\HD87S2NJ\Öhman - An audio-visual speech database and automatic meas.pdf}
}

@book{rochTethysWorkbenchBioacoustic2013,
  title = {Tethys: {{A}} Workbench for Bioacoustic Measurements and Environmental Data},
  shorttitle = {Tethys},
  author = {Roch, Marie and {Baumann-Pickering}, Simone and Hwang, Daniel and Batchelor, Heidi and Berchok, Catherine and Cholewiak, Danielle and Hildebrand, John and Munger, Lisa and Oleson, Erin and Rankin, Shannon and Risch, Denise and {\v S}irovi{\'c}, Ana and Soldevilla, Melissa and Van Parijs, Sofie},
  year = {2013},
  month = nov,
  journal = {The Journal of the Acoustical Society of America},
  volume = {134},
  doi = {10.1121/1.4831309},
  abstract = {A growing number of passive acoustic monitoring systems have resulted in a wealth of annotation information, or metadata, for recordings. These metadata are semi-structured. Some parameters are essentially mandatory (e.g., time of detection and what was detected) while others are highly dependent upon the question that a researcher is asking. Tethys is a metadata system for spatial-temporal acoustic data that provides structure where it is appropriate and flexibility where it is needed. Networked metadata are stored in an extended markup language (XML) database, and served to workstations over a network. The ability to export summary data to OBIS-SEAMAP is in development. The second purpose of Tethys is to serve as a scientific workbench. Interfaces are provided to networked databases, permitting the import of data from a wide variety of sources, such as lunar illumination or sea ice coverage. Interfaces currently exist for Matlab, Java, and Python. Writing data driven queries using a single interface enables quick data gathering from multivariate sources to address hypotheses. Examples showing the results of analysis of acoustic data from acoustic deployment from 26 sites across the Northern Pacific will be shown.},
  file = {C:\Users\daniel\Zotero\storage\QQNEU8YN\Roch et al. - 2013 - Tethys A workbench for bioacoustic measurements a.pdf}
}

@book{cassidyCompilingMultitieredSpeech1999,
  title = {Compiling Multi-Tiered Speech Databases into the Relational Model: {{Experiments}} with the {{EMU}} System},
  shorttitle = {Compiling Multi-Tiered Speech Databases into the Relational Model},
  author = {Cassidy, Steve},
  year = {1999},
  month = jan,
  file = {C:\Users\daniel\Zotero\storage\FV3NPTZX\Cassidy - 1999 - Compiling multi-tiered speech databases into the r.pdf}
}

@article{rochManagementAcousticMetadata2016,
  title = {Management of Acoustic Metadata for Bioacoustics},
  author = {Roch, Marie A. and Batchelor, Heidi and {Baumann-Pickering}, Simone and Berchok, Catherine L. and Cholewiak, Danielle and Fujioka, Ei and Garland, Ellen C. and Herbert, Sean and Hildebrand, John A. and Oleson, Erin M. and Van Parijs, Sofie and Risch, Denise and {\v S}irovi{\'c}, Ana and Soldevilla, Melissa S.},
  year = {2016},
  month = jan,
  journal = {Ecological Informatics},
  volume = {31},
  pages = {122--136},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2015.12.002},
  urldate = {2024-06-11},
  abstract = {Recent expansion in the capabilities of passive acoustic monitoring of sound-producing animals is providing expansive data sets in many locations. These long-term data sets will allow the investigation of questions related to the ecology of sound-producing animals on time scales ranging from diel and seasonal to inter-annual and decadal. Analyses of these data often span multiple analysts from various research groups over several years of effort and, as a consequence, have begun to generate large amounts of scattered acoustic metadata. It has therefore become imperative to standardize the types of metadata being generated. A critical aspect of being able to learn from such large and varied acoustic data sets is providing consistent and transparent access that can enable the integration of various analysis efforts. This is juxtaposed with the need to include new information for specific research questions that evolve over time. Hence, a method is proposed for organizing acoustic metadata that addresses many of the problems associated with the retention of metadata from large passive acoustic data sets. A structure was developed for organizing acoustic metadata in a consistent manner, specifying required and optional terms to describe acoustic information derived from a recording. A client-server database was created to implement this data representation as a networked data service that can be accessed from several programming languages. Support for data import from a wide variety of sources such as spreadsheets and databases is provided. The implementation was extended to access Internet-available data products, permitting access to a variety of environmental information types (e.g. sea surface temperature, sunrise/sunset, etc.) from a wide range of sources as if they were part of the data service. This metadata service is in use at several institutions and has been used to track and analyze millions of acoustic detections from marine mammals, fish, elephants, and anthropogenic sound sources.},
  keywords = {Bioacoustics,Call spatiotemporal database,Environmental data access,Metadata},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\J4QIBSKB\\Roch et al. - 2016 - Management of acoustic metadata for bioacoustics.pdf;C\:\\Users\\daniel\\Zotero\\storage\\YX9DJCKQ\\S1574954115001983.html}
}

@article{nashElectronicDatabaseSpeech2014,
  title = {An Electronic Database of Speech Sound Levels},
  author = {Nash, Anthony and Associates, Charles M Salter},
  year = {2014},
  abstract = {In early 1970's, the U.S. Environmental Protection Agency sponsored a research program involving speech sound levels in various noise environments. Bolt, Beranek and Newman (BB\&N) conducted this research program under contract. Among other tasks, BB\&N measured speech sound levels under controlled conditions from subjects using five categories of speech effort. In 1977, a summary report of the research program was prepared. This report was supported by a companion data supplement containing a statistical analysis of sound pressure levels measured at one meter from 97 individual subjects in an anechoic chamber. The data were obtained from a one-third-octave-band real-time analyzer, processed by a mini-computer, and then printed in the form of 482 cumulative statistical distributions. The 140,000 sound pressure levels in this printed data supplement have now been converted to an electronic database to create a convenient and efficient resource for the acoustical community. The data can be used for many purposes, including the prediction of speech privacy for a variety of speech efforts. This paper summarizes the contents of the database and discusses its possible applications.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\BAJM7NA6\Nash and Associates - 2014 - An electronic database of speech sound levels.pdf}
}

@article{hirschWEAPONDATABASEPREDICTION,
  title = {A {{WEAPON DATABASE FOR THE PREDICTION OF SHOOTING NOISE}}},
  author = {Hirsch, Karl-Wilhelm and Trimpop, Mattias},
  abstract = {The relational database structure WAF2000 provides an unique key to clearly define sources of blasts from military and civil weapons. The structure offers a multi-lingual management of names and descriptions for each entity of each table. This structure is public and available from the Institut fuer Laermschutz.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\432F6QE6\Hirsch and Trimpop - A WEAPON DATABASE FOR THE PREDICTION OF SHOOTING N.pdf}
}

@article{greenEigenScapeDatabaseSpatial2017,
  title = {{{EigenScape}}: {{A Database}} of {{Spatial Acoustic Scene Recordings}}},
  shorttitle = {{{EigenScape}}},
  author = {Green, Marc Ciufo and Murphy, Damian},
  year = {2017},
  month = nov,
  journal = {Applied Sciences},
  volume = {7},
  number = {11},
  pages = {1204},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app7111204},
  urldate = {2024-06-11},
  abstract = {The classification of acoustic scenes and events is an emerging area of research in the field of machine listening. Most of the research conducted so far uses spectral features extracted from monaural or stereophonic audio rather than spatial features extracted from multichannel recordings. This is partly due to the lack thus far of a substantial body of spatial recordings of acoustic scenes. This paper formally introduces EigenScape, a new database of fourth-order Ambisonic recordings of eight different acoustic scene classes. The potential applications of a spatial machine listening system are discussed before detailed information on the recording process and dataset are provided. A baseline spatial classification system using directional audio coding (DirAC) techniques is detailed and results from this classifier are presented. The classifier is shown to give good overall scene classification accuracy across the dataset, with 7 of 8 scenes being classified with an accuracy of greater than 60\% with an 11\% improvement in overall accuracy compared to use of Mel-frequency cepstral coefficient (MFCC) features. Further analysis of the results shows potential improvements to the classifier. It is concluded that the results validate the new database and show that spatial features can characterise acoustic scenes and as such are worthy of further investigation.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {acoustic environment,acoustic scene,ambisonics,dataset,Eigenmike,machine learning,recordings,soundscape,spatial audio},
  file = {C:\Users\daniel\Zotero\storage\XQFMYTJE\Green and Murphy - 2017 - EigenScape A Database of Spatial Acoustic Scene R.pdf}
}

@inproceedings{mesarosTUTDatabaseAcoustic2016,
  title = {{{TUT}} Database for Acoustic Scene Classification and Sound Event Detection},
  booktitle = {2016 24th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas},
  year = {2016},
  month = aug,
  pages = {1128--1132},
  publisher = {IEEE},
  address = {Budapest, Hungary},
  doi = {10.1109/EUSIPCO.2016.7760424},
  urldate = {2024-06-11},
  abstract = {We introduce TUT Acoustic Scenes 2016 database for environmental sound research, consisting of binaural recordings from 15 different acoustic environments. A subset of this database, called TUT Sound Events 2016, contains annotations for individual sound events, specifically created for sound event detection. TUT Sound Events 2016 consists of residential area and home environments, and is manually annotated to mark onset, offset and label of sound events. In this paper we present the recording and annotation procedure, the database content, a recommended cross-validation setup and performance of supervised acoustic scene classification system and event detection baseline system using mel frequency cepstral coefficients and Gaussian mixture models. The database is publicly released to provide support for algorithm development and common ground for comparison of different techniques.},
  isbn = {978-0-9928626-5-7},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\CAQXVHTI\Mesaros et al. - 2016 - TUT database for acoustic scene classification and.pdf}
}

@patent{hearingDroneDetectionClassification2016,
  title = {Drone Detection and Classification Methods and Apparatus},
  author = {Hearing, Brian and Franklin, John},
  year = {2016},
  month = mar,
  number = {US9275645B2},
  urldate = {2024-06-11},
  assignee = {Droneshield LLC},
  langid = {english},
  nationality = {US},
  keywords = {drone,frequency spectrum,processor,sample,sound},
  file = {C:\Users\daniel\Zotero\storage\A6BDZWUT\Hearing and Franklin - 2016 - Drone detection and classification methods and app.pdf}
}

@article{hauzenbergerDroneDetectionUsing2015,
  title = {Drone {{Detection}} Using {{Audio Analysis}}},
  author = {Hauzenberger, Louise and Holmberg Ohlsson, Emma},
  year = {2015},
  urldate = {2024-06-11},
  abstract = {Drones used for illegal purposes is a growing problem and a way to detect these is needed. This thesis has evaluated the possibility of using sound analysis as the detection mechanism. A solution using linear predictive coding, the slope of the frequency spectrum and the zero crossing rate was evaluated. The results showed that a solution using linear predictive coding and the slope of the frequency spectrum give a good result for the distance it is calibrated for. The zero crossing rate on the other hand does not improve the result and was not part of the final solution. The amount of false positives increases when calibrating for longer distances, and a compromise between detecting drones at long distances and the number of false positives need to be made in the implemented solution. It was concluded that drone detection using audio analysis is possible, and that the implemented solution, with linear predictive coding and slope of the frequency spectrum, could with further improvements become a useable product.},
  copyright = {info:eu-repo/semantics/embargoedAccess},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\E95FE6ZX\Hauzenberger and Holmberg Ohlsson - 2015 - Drone Detection using Audio Analysis.pdf}
}

@article{kadyrovPestPulseMultimodalStored2024,
  title = {{{PestPulse}}: {{A Multi-modal Stored Product Insect Dataset}} for {{Agriculture}}, {{Food}}, and {{Biosecurity}}},
  shorttitle = {{{PestPulse}}},
  author = {Kadyrov, Daniel and Zhao, Dan and Sedunov, Alexander and Sedunov, Nikolay and Salloum, Hady and Sutin, Alexander and Samsonau, Sergey V.},
  year = {2024},
  month = may,
  urldate = {2024-06-18},
  abstract = {Stored products such as grains, legumes, and nuts are foundational in the global food supply but are susceptible to infestation by insect pests including invasive ones. These stored product pests are responsible for 20{\textbackslash}\% of global food loss (1) and invasive species have been estimated to cost a minimum of 70 billion USD a year globally in agricultural damages, totaling over 1 trillion USD over the last 50 years (2). Current methods of inspection at ports of entry rely on manual inspection which is unreliable due to poor visibility, small pest size, material/product density, the sensitivity of the signals produced, and the volume of content requiring inspection. To help combat this problem, we introduce PestPulse, the Stored Product Insect Dataset (SPID)---a unique dataset consisting of 60+ hours of recordings featuring acoustic and microwave data from various insects within different materials and levels of background noise collected from specialized proprietary multichannel acoustic and microwave sensors. This dataset can be leveraged to iteratively refine sensor performance and sensitivity, support insect pest localization and classification, help improve detection technology capabilities, and encourage further work in multi-modal and audio machine learning both within and beyond this area. Our data and repository are available at (3;4)},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\EK4638S5\Kadyrov et al. - 2024 - PestPulse A Multi-modal Stored Product Insect Dat.pdf}
}

@inproceedings{rankinRadarImagingConventional2012,
  title = {Radar {{Imaging}}: {{Conventional}} and {{Mimo}}},
  shorttitle = {Radar {{Imaging}}},
  booktitle = {2012 {{Fourth International Conference}} on {{Communications}} and {{Electronics}} ({{ICCE}})},
  author = {Rankin, G. A. and Bui, L. Q. and Tirkel, A. Z. and Le Marshall, N.W.D.},
  year = {2012},
  month = aug,
  pages = {171--176},
  publisher = {IEEE},
  address = {Hue, Vietnam},
  doi = {10.1109/CCE.2012.6315892},
  urldate = {2024-07-19},
  abstract = {Our group has developed two diverse radar sensors with practical applications: an imaging radar at 94 GHz for Unmanned Aerial Vehicle (UAV) take-off and landing and a radar based termite detector at 24 GHz. These two sensors operate in vastly different environments. One relies on the weak radar returns from ground clutter to form a high fidelity image. The other detects the movement of termites hidden inside building materials at extremely short distances. Currently, our approach is moving away from classical radar towards Multiple Input Multiple Output (MIMO) radar. This paper presents the sensor implementations and recent results. The aircraft radar is still a demonstration system, whilst the termite detector is a mature product.},
  isbn = {978-1-4673-2493-9 978-1-4673-2492-2 978-1-4673-2491-5},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\LKB9SUQI\Rankin et al. - 2012 - Radar imaging Conventional and MIMO.pdf}
}

@misc{PiezoelectricDiscDiameter2024,
  title = {Piezoelectric {{Disc}}, {{Diameter}} 27 Mm, {{Regular Thickness}} 0.33 Mm},
  year = {2024},
  urldate = {2024-07-19},
  howpublished = {https://auroraprosci.com/piezoelectric-disc-diameter-27-mm-regular-thickness-0.33-mm},
  file = {C:\Users\daniel\Zotero\storage\EUDILDEV\piezoelectric-disc-diameter-27-mm-regular-thickness-0.html}
}

@article{smithEfficiencyBerleseTullgrenFunnels1977,
  title = {Efficiency of {{Berlese-Tullgren Funnels}} for {{Removal}} of the {{Rusty Grain Beetle}},},
  author = {Smith, L. B.},
  year = {1977},
  month = apr,
  journal = {The Canadian Entomologist},
  volume = {109},
  number = {4},
  pages = {503--509},
  issn = {1918-3240, 0008-347X},
  doi = {10.4039/Ent109503-4},
  urldate = {2024-07-23},
  abstract = {The efficiency of Berlese-Tullgren funnels in removing the rusty grain beetle, Cryptolestes ferrugineus (Stephens), from wheat samples was tested by adding adults and larvae to wheat samples. The recovery of adults from 300 g samples was 79\% at 12.3\% moisture content (m.c.) and 49\% at 16\% m.c.; from 150 g samples the recovery was 84\% at 16\% m.c. The number of adults recovered represented at least 98\% of those that did not escape from the top of the samples. The number that escaped varied with sample size and moisture content. The recovery of fourth instar larvae was 78\% from samples of 300 g at 16\% m.c. The percentage of first and second instar larvae recovered was 5.8 when larvae alone were mixed with the wheat sample and left for 65 h, 10.6 when larvae in flour were added to the surface of the wheat sample, and 27.9 when larvae in flour were added to the centre of the wheat sample.},
  langid = {english}
}

@patent{litzkowElectronicGrainProbe1997,
  title = {Electronic Grain Probe Insect Counter ({{EGPIC}})},
  author = {Litzkow, Carl A. and Shuman, Dennis and Kruss, Sergey and Coffelt, James A.},
  year = {1997},
  month = jul,
  number = {US5646404A},
  urldate = {2024-07-23},
  assignee = {US Department of Agriculture USDA},
  nationality = {US},
  keywords = {body section,computer,insect,probe,sensor head},
  file = {C:\Users\daniel\Zotero\storage\ELJQ8WJ9\Litzkow et al. - 1997 - Electronic grain probe insect counter (EGPIC).pdf}
}

@article{bonjourEvaluatingRemoteMonitoring2004,
  title = {Evaluating a {{Remote Monitoring Device}} for {{Stored Grain Insects}} in a {{Commercial Facility}}},
  author = {Bonjour, Edmond and Phillips, Thomas and LARSON, {\relax RON} and Shuman, Dennis},
  year = {2004},
  month = jan,
  abstract = {The StorMax Insector is an electronic device for remotely monitoring insects in stored grain. Arthropods migrating within the grain mass encounter the probe trap body and are electronically counted by a computer as they fall through and interrupt infrared beams. A study in two large commercial steel bins containing stored wheat was conducted in Oklahoma to determine the accuracy of electronic counts recorded by Insector compared to actual counts of arthropods collected in the probe collecting tip. Electronic counts were approximately two times higher than actual counts over a range of counts. The probable cause of overestimation was the extremely high number of insects in the family Psocidae that entered the traps. Refinement of the discriminating values entered into the software program will be able to filter out counts of these small insects and remove them from the reported counts to make the estimate more accurate for stored grain beetles counted. Insectors with collecting tips with holes near the bottom for insect release allowed insects to escape after passing through the sensor, including the high number of Psocidae. Using Insector technology to monitor insect activity will determine when treatment is necessary and improve safety by reducing the need for workers to enter grain bins.},
  file = {C:\Users\daniel\Zotero\storage\9JCYQ5SN\Bonjour et al. - 2004 - Evaluating a Remote Monitoring Device for Stored G.pdf}
}

@patent{shadeUltrasonicInsectDetector1989,
  title = {Ultrasonic {{Insect Detector}}},
  author = {Shade, Richard E. and Furgason, Eric S. and Murdock, Larry L.},
  year = {1989},
  month = mar,
  number = {US4809554A},
  urldate = {2024-07-23},
  assignee = {Purdue Research Foundation},
  nationality = {US},
  keywords = {coupled,envelope,feeding,output,signals},
  file = {C:\Users\daniel\Zotero\storage\63TQRN82\Shade et al. - 1989 - Ultrasonic insect detector.pdf}
}

@article{tranConsequencesInbreedingCowpea1995,
  title = {Consequences of {{Inbreeding}} for the {{Cowpea Seed Beetle}}, {{Callosobruchus Maculatus}} (f.) (Coleoptera: {{Bruchidae}})},
  shorttitle = {Consequences of {{Inbreeding}} for the {{Cowpea Seed Beetle}}, {{Callosobruchus Maculatus}} (f.) (Coleoptera},
  author = {Tran, Burno M.D. and Credland, Peter F.},
  year = {1995},
  month = nov,
  journal = {Biological Journal of the Linnean Society},
  volume = {56},
  number = {3},
  pages = {483--503},
  issn = {0024-4066},
  doi = {10.1111/j.1095-8312.1995.tb01106.x},
  urldate = {2024-07-23},
  abstract = {Inbreeding is said to reduce vigour and fitness. It may also determine how a population responds to selection. Local populations of Callosobruchus maculatus, the cowpea seed beetle, are established annually from small numbers of founders and the species has been distributed to many parts of the world where isolated populations may have been founded by very small numbers of individuals. After more than 20 generations of inbreeding, inbred lines have been shown to diverge from a common ancestral stock in similar directions with respect of some variables such as developmental speed, but haphazardly in respect of other parameters such as male weight. The respective roles of drift and of selection as effective evolutionary forces in inbred lines are discussed in the light of these results. It is argued that some intraspecific differences in C. maculatus may be explained as a product of periodic inbreeding, but that the process does not impair the ability to adapt to local conditions so contributing to the status of the species as a pest of international importance.}
}

@misc{registry-migration.gbif.orgAnimalSoundArchive2016,
  title = {Animal {{Sound Archive}}},
  author = {{Registry-Migration.Gbif.Org}},
  year = {2016},
  publisher = {Museum f{\"u}r Naturkunde Berlin},
  doi = {10.15468/0BPALR},
  urldate = {2024-07-23},
  abstract = {The Animal Sound Archive at the Museum fuer Naturkunde Berlin (German: Tierstimmenarchiv) is one of the oldest and largest worldwide. Founded in 1951 by Professor Guenter Tembrock the collection consists now of around 130 000 records of animal voices.},
  copyright = {CC Licenses (see records)}
}

@misc{BuildingAIApplicationsa,
  title = {Building {{AI Applications Made Easy}}},
  journal = {DagsHub},
  urldate = {2024-08-10},
  abstract = {DagsHub simplifies the process of building better models and managing unstructured data projects in one place.},
  howpublished = {https://dagshub.com/},
  langid = {american}
}

@article{ulloaScikitmaadOpensourceModular2021,
  title = {Scikit-Maad: {{An}} Open-Source and Modular Toolbox for Quantitative Soundscape Analysis in {{Python}}},
  shorttitle = {Scikit-Maad},
  author = {Ulloa, Juan Sebasti{\'a}n and Haupert, Sylvain and Latorre, Juan Felipe and Aubin, Thierry and Sueur, J{\'e}r{\^o}me},
  year = {2021},
  journal = {Methods in Ecology and Evolution},
  volume = {12},
  number = {12},
  pages = {2334--2340},
  issn = {2041-210X},
  doi = {10.1111/2041-210X.13711},
  urldate = {2024-08-10},
  abstract = {Passive acoustic monitoring is increasingly being applied to terrestrial, marine and freshwater environments, providing cost-efficient methods for surveying biodiversity. However, processing the avalanche of audio recordings remains challenging, and represents nowadays a major bottleneck that slows down its application in research and conservation. We present scikit-maad, an open-source Python package dedicated to the analysis of environmental audio recordings. This package was designed to (a) load and process digital audio, (b) segment and find regions of interest, (c) compute acoustic features and (d) estimate sound pressure levels. The package also provides field recordings and a comprehensive online documentation that includes practical examples with step-by-step instructions for beginners and advanced users. scikit-maad opens the possibility to efficiently scan large audio datasets and easily integrate additional machine learning Python packages into the analysis, allowing to measure acoustic properties and identify key patterns in all kinds of soundscapes. To support reproducible research, the package is released under the BSD open-source licence, which allows unrestricted redistribution for commercial and private use. This development will create synergies between the community of ecoacousticians, such as engineers, data scientists, ecologists, biologists and conservation practitioners, to explore and understand the processes underlying the acoustic diversity of ecological systems.},
  langid = {english},
  keywords = {acoustic indices,bioacoustics,ecoacoustics,pattern recognition,sound pressure level},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\8QULBZCG\\Ulloa et al. - 2021 - scikit-maad An open-source and modular toolbox fo.pdf;C\:\\Users\\daniel\\Zotero\\storage\\V3F5VVR5\\2041-210X.html}
}

@article{bogdanovESSENTIAAUDIOANALYSIS,
  title = {{{ESSENTIA}}: {{AN AUDIO ANALYSIS LIBRARY FOR MUSIC INFORMATION RETRIEVAL}}},
  author = {Bogdanov, Dmitry and Wack, Nicolas and Gomez, Emilia and Gulati, Sankalp and Herrera, Perfecto and Mayor, Oscar and Roma, Gerard and Salamon, Justin and Zapata, Jose and Serra, Xavier},
  abstract = {We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predefined executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, specifically the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\VFLBURDX\Bogdanov et al. - ESSENTIA AN AUDIO ANALYSIS LIBRARY FOR MUSIC INFO.pdf}
}

@article{giannakopoulosPyAudioAnalysisOpenSourcePython2015,
  title = {{{pyAudioAnalysis}}: {{An Open-Source Python Library}} for {{Audio Signal Analysis}}},
  shorttitle = {{{pyAudioAnalysis}}},
  author = {Giannakopoulos, Theodoros},
  editor = {Pavan, Gianni},
  year = {2015},
  month = dec,
  journal = {PLOS ONE},
  volume = {10},
  number = {12},
  pages = {e0144610},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0144610},
  urldate = {2024-08-10},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\5GVFSM6T\Giannakopoulos - 2015 - pyAudioAnalysis An Open-Source Python Library for.pdf}
}

@article{giannakopoulosPyAudioAnalysisOpenSourcePython2015a,
  title = {{{pyAudioAnalysis}}: {{An Open-Source Python Library}} for {{Audio Signal Analysis}}},
  shorttitle = {{{pyAudioAnalysis}}},
  author = {Giannakopoulos, Theodoros},
  year = {2015},
  month = dec,
  journal = {PLOS ONE},
  volume = {10},
  number = {12},
  pages = {e0144610},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0144610},
  urldate = {2024-08-10},
  abstract = {Audio information plays a rather important role in the increasing digital content that is available today, resulting in a need for methodologies that automatically analyze such content: audio event recognition for home automations and surveillance systems, speech recognition, music information retrieval, multimodal analysis (e.g. audio-visual analysis of online videos for content-based recommendation), etc. This paper presents pyAudioAnalysis, an open-source Python library that provides a wide range of audio analysis procedures including: feature extraction, classification of audio signals, supervised and unsupervised segmentation and content visualization. pyAudioAnalysis is licensed under the Apache License and is available at GitHub (https://github.com/tyiannak/pyAudioAnalysis/). Here we present the theoretical background behind the wide range of the implemented methodologies, along with evaluation metrics for some of the methods. pyAudioAnalysis has been already used in several audio analysis research applications: smart-home functionalities through audio event detection, speech emotion recognition, depression classification based on audio-visual features, music segmentation, multimodal content-based movie recommendation and health applications (e.g. monitoring eating habits). The feedback provided from all these particular audio applications has led to practical enhancement of the library.},
  langid = {english},
  keywords = {Acoustic signals,Audio equipment,Audio signal processing,Hidden Markov models,Libraries,Library screening,Machine learning,Speech signal processing},
  file = {C:\Users\daniel\Zotero\storage\ZU2S8P2J\Giannakopoulos - 2015 - pyAudioAnalysis An Open-Source Python Library for.pdf}
}

@misc{groverAudinoModernAnnotation2021,
  title = {Audino: {{A Modern Annotation Tool}} for {{Audio}} and {{Speech}}},
  shorttitle = {Audino},
  author = {Grover, Manraj Singh and Bamdev, Pakhi and Brala, Ratin Kumar and Kumar, Yaman and Hama, Mika and Shah, Rajiv Ratn},
  year = {2021},
  month = nov,
  number = {arXiv:2006.05236},
  eprint = {2006.05236},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2024-08-10},
  abstract = {In this paper, we introduce a collaborative and modern annotation tool for audio and speech: audino. The tool allows annotators to define and describe temporal segmentation in audios. These segments can be labelled and transcribed easily using a dynamically generated form. An admin can centrally control user roles and project assignment through the admin dashboard. The dashboard also enables describing labels and their values. The annotations can easily be exported in JSON format for further analysis. The tool allows audio data and their corresponding annotations to be uploaded and assigned to a user through a key-based API. The flexibility available in the annotation tool enables annotation for Speech Scoring, Voice Activity Detection (VAD), Speaker Diarisation, Speaker Identification, Speech Recognition, Emotion Recognition tasks and more. The MIT open source license allows it to be used for academic and commercial projects.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C:\Users\daniel\Zotero\storage\WCXTAYNZ\Grover et al. - 2021 - audino A Modern Annotation Tool for Audio and Spe.pdf}
}

@inproceedings{stastnyAudioDataClassification2013,
  title = {Audio Data Classification by Means of New Algorithms},
  booktitle = {2013 36th {{International Conference}} on {{Telecommunications}} and {{Signal Processing}} ({{TSP}})},
  author = {Stastny, Jiri and Skorpil, Vladislav and Fejfar, Jiri},
  year = {2013},
  month = jul,
  pages = {507--511},
  doi = {10.1109/TSP.2013.6613984},
  urldate = {2024-08-10},
  abstract = {This paper describes classification of sound recordings based on their audio features. This is useful for querying large datasets, searching for recordings with some desired content. We use musical recordings as well as birdsongs recordings, which usually have rich structure and contain a lot of patterns suitable for classification. We present two different classification methods, one for musical recordings and one for birdsongs. These methods are compared and their differences are discussed. We use feature vectors that capture the audio content of recording as a whole piece and then classify these feature vectors using combination of the Self-organizing map and the Learning Vector Quantization, which represent a powerful algorithm using unlabeled as well as labeled data. In case of birdsongs we use feature vectors representing time frames of a recording.},
  keywords = {Accuracy,Birds,classification,Clustering algorithms,Hidden Markov models,HMM,LVQ,Neurons,semi-supervised learning,SOM,sound processing,Vector quantization,Vectors},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\J7YKC3NL\\Stastny et al. - 2013 - Audio data classification by means of new algorith.pdf;C\:\\Users\\daniel\\Zotero\\storage\\X5UN72NA\\6613984.html}
}

@article{linNEWAPPROACHCLASSIFICATION2005,
  title = {A {{NEW APPROACH FOR CLASSIFICATION OF GENERIC AUDIO DATA}}},
  author = {Lin, Ruei-Shiang and Chen, Ling-Hwei},
  year = {2005},
  month = feb,
  journal = {International Journal of Pattern Recognition and Artificial Intelligence},
  volume = {19},
  number = {01},
  pages = {63--78},
  issn = {0218-0014, 1793-6381},
  doi = {10.1142/S0218001405003958},
  urldate = {2024-08-10},
  abstract = {The existing audio retrieval systems fall into one of two categories: single-domain systems that can accept data of only a single type (e.g. speech) or multiple-domain systems that offer content-based retrieval for multiple types of audio data. Since a single-domain system has limited applications, a multiple-domain system will be more useful. However, different types of audio data will have different properties, this will make a multiple-domain system harder to be developed. If we can classify audio information in advance, the above problems can be solved. In this paper, we will propose a real-time classification method to classify audio signals into several basic audio types such as pure speech, music, song, speech with music background, and speech with environmental noise background.             In order to make the proposed method robust for a variety of audio sources, we use Bayesian decision function for multivariable Gaussian distribution instead of manually adjusting a threshold for each discriminator. The proposed approach can be applied to content-based audio/video retrieval. In the experiment, the efficiency and effectiveness of this method are shown by an accuracy rate of more than 96\% for general audio data classification.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\DNBIZSMQ\Lin and Chen - 2005 - A NEW APPROACH FOR CLASSIFICATION OF GENERIC AUDIO.pdf}
}

@article{hirschbergStudyingSearchArchiving,
  title = {Studying {{Search}} and {{Archiving}} in a {{Real Audio Database}}},
  author = {Hirschberg, Julia and Whitaker, Steve},
  abstract = {Technological advances in storage, indexing and search technologies are increasingly making it possible to archive and retrieve multimediamaterials [3,4,6,8]. Thefocus of muchcurrent workhas been on the development of enabling technologies to allow users access to text, image, video and audio databases. It is clearly important however, that such technology developmentbe informed by an understanding of user and application requirements. Our study attempts to documenthowpeople use, store, search and process large amountsof audio data in a real setting: in order to study real users with real audio retrieval problems we examined heavy users of a commercial voicemail system. By analyzing qualititative and quantitative usage data, we were able to derive a numberof design requirementsfor multimediaaccess in general, as well as specific design principles for voice messagingand archiving systems.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\CDQST535\Hirschberg and Whitaker - Studying Search and Archiving in a Real Audio Data.pdf}
}

@book{wieczorkowskaAudioContentDescription2001,
  title = {Audio {{Content Description}} in {{Sound Databases}}},
  author = {Wieczorkowska, Alicja and Ras, Zbigniew},
  year = {2001},
  month = jan,
  volume = {2198},
  pages = {183},
  doi = {10.1007/3-540-45490-X_20},
  abstract = {Sound database indexing requires metadata to represent audio content of the data. If the metadata are not attached to the database by its creator, content information has to be extracted directly from sounds, using descriptors based on sound analysis. In this paper, authors present a number of sound descriptors based on various forms of signal analysis. Telescope Vector trees (TV-trees) and Frame Segment trees (FS-trees) are applied to represent audio content on the basis of the extracted sound descriptors and metadata provided by the database creator (if only available). Such a representation of audio content of the database is used to speed up the search of the audio material in multimedia databases.},
  isbn = {978-3-540-42730-8}
}

@inproceedings{hadadMultichannelAudioDatabase2014a,
  title = {Multichannel Audio Database in Various Acoustic Environments},
  booktitle = {2014 14th {{International Workshop}} on {{Acoustic Signal Enhancement}} ({{IWAENC}})},
  author = {Hadad, Elior and Heese, Florian and Vary, Peter and Gannot, Sharon},
  year = {2014},
  month = sep,
  pages = {313--317},
  doi = {10.1109/IWAENC.2014.6954309},
  urldate = {2024-08-10},
  abstract = {In this paper we describe a new multichannel room impulse responses database. The impulse responses are measured in a room with configurable reverberation level resulting in three different acoustic scenarios with reverberation times RT60 equals to 160 ms, 360 ms and 610 ms. The measurements were carried out in recording sessions of several source positions on a spatial grid (angle range of -90{$^\circ$} to 90{$^\circ$} in 15{$^\circ$} steps with 1 m and 2 m distance from the microphone array). The signals in all sessions were captured by three microphone array configurations. The database is accompanied with software utilities to easily access and manipulate the data. Besides the description of the database we demonstrate its use in spatial source separation task.},
  keywords = {Acoustic measurements,Arrays,Database,Databases,microphone arrays,Microphones,multi-channel,Reverberation,room impulse response,Speech},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\IYRDE4JR\\Hadad et al. - 2014 - Multichannel audio database in various acoustic en.pdf;C\:\\Users\\daniel\\Zotero\\storage\\AMGYV9RM\\6954309.html}
}

@article{jadoulIntroducingParselmouthPython2018,
  title = {Introducing {{Parselmouth}}: {{A Python}} Interface to {{Praat}}},
  shorttitle = {Introducing {{Parselmouth}}},
  author = {Jadoul, Yannick and Thompson, Bill and {de Boer}, Bart},
  year = {2018},
  month = nov,
  journal = {Journal of Phonetics},
  volume = {71},
  pages = {1--15},
  issn = {0095-4470},
  doi = {10.1016/j.wocn.2018.07.001},
  urldate = {2024-08-10},
  abstract = {This paper introduces Parselmouth, an open-source Python library that facilitates access to core functionality of Praat in Python, in an efficient and programmer-friendly way. We introduce and motivate the package, and present simple usage examples. Specifically, we focus on applications in data visualisation, file manipulation, audio manipulation, statistical analysis, and integration of Parselmouth into a Python-based experimental design for automated, in-the-loop manipulation of acoustic data. Parselmouth is available at https://github.com/YannickJadoul/Parselmouth.},
  keywords = {Acoustics,Data analysis,Phonetics,Praat,Python,Software},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\VEAJ4VXB\\Jadoul et al. - 2018 - Introducing Parselmouth A Python interface to Pra.pdf;C\:\\Users\\daniel\\Zotero\\storage\\UUXI5SSR\\S0095447017301389.html}
}

@inproceedings{alonso-jimenezTensorflowAudioModels2020,
  title = {Tensorflow {{Audio Models}} in {{Essentia}}},
  booktitle = {{{ICASSP}} 2020 - 2020 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {{Alonso-Jim{\'e}nez}, Pablo and Bogdanov, Dmitry and Pons, Jordi and Serra, Xavier},
  year = {2020},
  month = may,
  pages = {266--270},
  issn = {2379-190X},
  doi = {10.1109/ICASSP40776.2020.9054688},
  urldate = {2024-08-10},
  abstract = {Essentia is a reference open-source C++/Python library for audio and music analysis. In this work, we present a set of algorithms that employ TensorFlow in Essentia, allow predictions with pre-trained deep learning models, and are designed to offer flexibility of use, easy extensibility, and real-time inference. To show the potential of this new interface with TensorFlow, we provide a number of pre-trained state-of-the-art music tagging and classification CNN models. We run an extensive evaluation of the developed models. In particular, we assess the generalization capabilities in a cross-collection evaluation utilizing both external tag datasets as well as manual annotations tailored to the taxonomies of our models.},
  keywords = {Analytical models,audio analysis software,Computational modeling,deep learning,Deep learning,Libraries,music information retrieval,music tagging,Python,Real-time systems,Tagging,transfer learning},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\QPZJKRUB\\Alonso-Jiménez et al. - 2020 - Tensorflow Audio Models in Essentia.pdf;C\:\\Users\\daniel\\Zotero\\storage\\MTCZTBV2\\9054688.html}
}

@article{goninaScalableMultimediaContent2014,
  title = {Scalable Multimedia Content Analysis on Parallel Platforms Using Python},
  author = {Gonina, Ekaterina and Friedland, Gerald and Battenberg, Eric and Koanantakool, Penporn and Driscoll, Michael and Georganas, Evangelos and Keutzer, Kurt},
  year = {2014},
  month = feb,
  journal = {ACM Transactions on Multimedia Computing, Communications, and Applications},
  volume = {10},
  number = {2},
  pages = {1--22},
  issn = {1551-6857, 1551-6865},
  doi = {10.1145/2517151},
  urldate = {2024-08-10},
  abstract = {In this new era dominated by consumer-produced media there is a high demand for web-scalable solutions to multimedia content analysis. A compelling approach to making applications scalable is to explicitly map their computation onto parallel platforms. However, developing efficient parallel implementations and fully utilizing the available resources remains a challenge due to the increased code complexity, limited portability and required low-level knowledge of the underlying hardware. In this article, we present PyCASP, a Python-based framework that automatically maps computation onto parallel platforms from Python application code to a variety of parallel platforms. PyCASP is designed using a systematic, pattern-oriented approach to offer a single software development environment for multimedia content analysis applications. Using PyCASP, applications can be prototyped in a couple hundred lines of Python code and automatically scale to modern parallel processors. Applications written with PyCASP are portable to a variety of parallel platforms and efficiently scale from a single desktop Graphics Processing Unit (GPU) to an entire cluster with a small change to application code. To illustrate our approach, we present three multimedia content analysis applications that use our framework: a state-of-the-art speaker diarization application, a content-based music recommendation system based on the Million Song Dataset, and a video event detection system for consumer-produced videos. We show that across this wide range of applications, our approach achieves the goal of automatic portability and scalability while at the same time allowing easy prototyping in a high-level language and efficient performance of low-level optimized code.},
  langid = {english}
}

@misc{lenainSurfboardAudioFeature2020,
  title = {Surfboard: {{Audio Feature Extraction}} for {{Modern Machine Learning}}},
  shorttitle = {Surfboard},
  author = {Lenain, Raphael and Weston, Jack and Shivkumar, Abhishek and Fristed, Emil},
  year = {2020},
  month = may,
  number = {arXiv:2005.08848},
  eprint = {2005.08848},
  primaryclass = {cs, eess},
  publisher = {arXiv},
  urldate = {2024-08-10},
  abstract = {We introduce Surfboard, an open-source Python library for extracting audio features with application to the medical domain. Surfboard is written with the aim of addressing pain points of existing libraries and facilitating joint use with modern machine learning frameworks. The package can be accessed both programmatically in Python and via its command line interface, allowing it to be easily integrated within machine learning workflows. It builds on state-of-the-art audio analysis packages and offers multiprocessing support for processing large workloads. We review similar frameworks and describe Surfboard's architecture, including the clinical motivation for its features. Using the mPower dataset, we illustrate Surfboard's application to a Parkinson's disease classification task, highlighting common pitfalls in existing research. The source code is opened up to the research community to facilitate future audio research in the clinical domain.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  file = {C:\Users\daniel\Zotero\storage\FMFQIQLZ\Lenain et al. - 2020 - Surfboard Audio Feature Extraction for Modern Mac.pdf}
}

@misc{HB100_Microwave_Sensor_datasheetPdf,
  title = {{{HB100}}\_{{Microwave}}\_{{Sensor}}\_datasheet.Pdf},
  urldate = {2024-06-08},
  howpublished = {https://theorycircuit.com/wp-content/uploads/2016/09/HB100\_Microwave\_Sensor\_datasheet.pdf},
  file = {C:\Users\daniel\Zotero\storage\8LYSHSEG\HB100_Microwave_Sensor_datasheet.pdf}
}

@article{AutomaticMotionDetection2023,
  title = {Automatic {{Motion Detection}} and {{Distance Measure Using Doppler Radar}}},
  year = {2023},
  month = jul,
  journal = {International Journal of Emerging Trends in Engineering Research},
  volume = {11},
  number = {7},
  pages = {236--239},
  issn = {23473983},
  doi = {10.30534/ijeter/2023/011172023},
  urldate = {2024-08-17},
  abstract = {The proposed device is a versatile and multi-functional system that uses an HB100 Doppler radar sensor and Arduino microcontroller to detect motion, measure velocity, and distance. The device is capable of detecting motion in realtime, measuring the velocity of the object, and calculating the distance between the object and the sensor.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\XLU5F33G\2023 - Automatic Motion Detection and Distance Measure Us.pdf}
}

@article{parsonsSoundingCallGlobal2022,
  title = {Sounding the {{Call}} for a {{Global Library}} of {{Underwater Biological Sounds}}},
  author = {Parsons, Miles J. G. and Lin, Tzu-Hao and Mooney, T. Aran and Erbe, Christine and Juanes, Francis and Lammers, Marc and Li, Songhai and Linke, Simon and Looby, Audrey and Nedelec, Sophie L. and Van Opzeeland, Ilse and Radford, Craig and Rice, Aaron N. and Sayigh, Laela and Stanley, Jenni and Urban, Edward and Di Iorio, Lucia},
  year = {2022},
  month = feb,
  journal = {Frontiers in Ecology and Evolution},
  volume = {10},
  pages = {810156},
  issn = {2296-701X},
  doi = {10.3389/fevo.2022.810156},
  urldate = {2024-08-19},
  abstract = {Aquatic environments encompass the world's most extensive habitats, rich with sounds produced by a diversity of animals. Passive acoustic monitoring (PAM) is an increasingly accessible remote sensing technology that uses hydrophones to listen to the underwater world and represents an unprecedented, non-invasive method to monitor underwater environments. This information can assist in the delineation of biologically important areas via detection of sound-producing species or characterization of ecosystem type and condition, inferred from the acoustic properties of the local soundscape. At a time when worldwide biodiversity is in significant decline and underwater soundscapes are being altered as a result of anthropogenic impacts, there is a need to document, quantify, and understand biotic sound sources--potentially before they disappear. A significant step toward these goals is the development of a web-based, open-access platform that provides: (1) a reference library of known and unknown biological sound sources (by integrating and expanding existing libraries around the world); (2) a data repository portal for annotated and unannotated audio recordings of single sources and of soundscapes; (3) a training platform for artificial intelligence algorithms for signal detection and classification; and (4) a citizen science-based application for public users. Although individually, these resources are often met on regional and taxa-specific scales, many are not sustained and, collectively, an enduring global database with an integrated platform has not been realized. We discuss the benefits such a program can provide, previous calls for global data-sharing and reference libraries, and the challenges that need to be overcome to bring together bio- and ecoacousticians, bioinformaticians, propagation experts, web engineers, and signal processing specialists (e.g., artificial intelligence) with the necessary support and funding to build a sustainable and scalable platform that could address the needs of all contributors and stakeholders into the future.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\L2KT5ELX\Parsons et al. - 2022 - Sounding the Call for a Global Library of Underwat.pdf}
}

@article{loobyFishSoundsVersionWebsite2023a,
  title = {{{FishSounds Version}} 1.0: {{A}} Website for the Compilation of Fish Sound Production Information and Recordings},
  shorttitle = {{{FishSounds Version}} 1.0},
  author = {Looby, Audrey and Vela, Sarah and Cox, Kieran and Riera, Amalis and Bravo, Santiago and Davies, Hailey L. and Rountree, Rodney and Reynolds, Laura K. and Martin, Charles W. and Matwin, Stan and Juanes, Francis},
  year = {2023},
  month = may,
  journal = {Ecological Informatics},
  volume = {74},
  pages = {101953},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2022.101953},
  urldate = {2024-08-22},
  abstract = {Many fish species use active sound production for communication in numerous behaviors. Additionally, likely all fish can make passive or incidental sounds that may also serve some signal functions. Despite the ecological importance of fish sounds, their evident passive acoustic monitoring applications, and extensive endeavors to document soniferous fish diversity, the fields of bioacoustics and ichthyology have historically lacked an easily accessible, global inventory of known fish sound production. To alleviate this limitation, we developed http://FishSounds.net, a website that compiles and disseminates fish sound production information and recordings. FishSounds Version 1.0 launched in 2021, cataloging documented examinations for active and passive sound production for 1185 fish species from 837 references as well as 239 exemplary audio recordings. FishSounds allows users to search by taxa (e.g., family or common name), geographical distribution (e.g., region or water body), sound type, or reference. We have also made available the code used to create the website, so that it may be used in other data-sharing efforts---acoustic or otherwise. Subsequent versions of the website will update the data and improve the website functionality. FishSounds will advance research into fish behavior, passive acoustic monitoring, and human impacts on underwater soundscapes; serve as a resource for public outreach; and provide the foundation needed to investigate more of the 96\% of fish species that lack published examinations of sound production. We further hope the FishSounds design, implementation, and engagement strategies will serve as a model for future data management and sharing efforts.},
  keywords = {Bioacoustics,Data portal,Fish behavior,Passive acoustic monitoring,Soundscape ecology},
  file = {C:\Users\daniel\Zotero\storage\EHGUJ69P\S1574954122004034.html}
}

@article{loobyFishSoundsVersionWebsite2023,
  title = {{{FishSounds Version}} 1.0: {{A}} Website for the Compilation of Fish Sound Production Information and Recordings},
  shorttitle = {{{FishSounds Version}} 1.0},
  author = {Looby, Audrey and Vela, Sarah and Cox, Kieran and Riera, Amalis and Bravo, Santiago and Davies, Hailey L. and Rountree, Rodney and Reynolds, Laura K. and Martin, Charles W. and Matwin, Stan and Juanes, Francis},
  year = {2023},
  month = may,
  journal = {Ecological Informatics},
  volume = {74},
  pages = {101953},
  issn = {1574-9541},
  doi = {10.1016/j.ecoinf.2022.101953},
  urldate = {2024-08-22},
  abstract = {Many fish species use active sound production for communication in numerous behaviors. Additionally, likely all fish can make passive or incidental sounds that may also serve some signal functions. Despite the ecological importance of fish sounds, their evident passive acoustic monitoring applications, and extensive endeavors to document soniferous fish diversity, the fields of bioacoustics and ichthyology have historically lacked an easily accessible, global inventory of known fish sound production. To alleviate this limitation, we developed http://FishSounds.net, a website that compiles and disseminates fish sound production information and recordings. FishSounds Version 1.0 launched in 2021, cataloging documented examinations for active and passive sound production for 1185 fish species from 837 references as well as 239 exemplary audio recordings. FishSounds allows users to search by taxa (e.g., family or common name), geographical distribution (e.g., region or water body), sound type, or reference. We have also made available the code used to create the website, so that it may be used in other data-sharing efforts---acoustic or otherwise. Subsequent versions of the website will update the data and improve the website functionality. FishSounds will advance research into fish behavior, passive acoustic monitoring, and human impacts on underwater soundscapes; serve as a resource for public outreach; and provide the foundation needed to investigate more of the 96\% of fish species that lack published examinations of sound production. We further hope the FishSounds design, implementation, and engagement strategies will serve as a model for future data management and sharing efforts.},
  keywords = {Bioacoustics,Data portal,Fish behavior,Passive acoustic monitoring,Soundscape ecology},
  file = {C:\Users\daniel\Zotero\storage\H52UIZT5\S1574954122004034.html}
}

@article{darrasEcoSoundwebOpensourceOnline2023,
  title = {{{ecoSound-web}}: An Open-Source, Online Platform for Ecoacoustics},
  shorttitle = {{{ecoSound-web}}},
  author = {Darras, Kevin F.A. and P{\'e}rez, Noem{\'i} and {-}, Mauladi and Dilong, Liu and {Hanf-Dressler}, Tara and Markolf, Matthias and Wanger, Thomas C},
  year = {2023},
  month = mar,
  journal = {F1000Research},
  volume = {9},
  pages = {1224},
  issn = {2046-1402},
  doi = {10.12688/f1000research.26369.2},
  urldate = {2024-08-22},
  abstract = {Passive acoustic monitoring of soundscapes and biodiversity produces vast amounts of audio recordings, but the management and analysis of these raw data present technical challenges. A multitude of software solutions exist, but none can fulfil all purposes required for the management, processing, navigation, analysis, and dissemination of acoustic data. The field of ecoacoustics needs a software tool that is free, evolving, and accessible. We take a step in that direction and present ecoSound-web: an open-source, online platform for ecoacoustics designed and built by ecologists and software engineers. ecoSound-web can be used for storing, organising, and sharing soundscape projects, manually creating and peer-reviewing annotations of soniferous animals and phonies, analysing audio in time and frequency, computing alpha acoustic indices, and providing reference sound libraries for different taxa. We present ecoSound-web's features, structure, and compare it with similar software. We describe its operation mode and the workflow for typical use cases such as the sampling of bird and bat communities, the use of a primate call library, and the analysis of phonies and acoustic indices. ecoSound-web is available from: https://github.com/ecomontec/ecoSound-web},
  pmcid = {PMC7682500},
  pmid = {33274051},
  file = {C:\Users\daniel\Zotero\storage\G9HWYQ85\Darras et al. - 2023 - ecoSound-web an open-source, online platform for .pdf}
}

@misc{darrasWorldwideSoundscapesSynthesis2024,
  title = {Worldwide {{Soundscapes}}: A Synthesis of Passive Acoustic Monitoring across Realms},
  shorttitle = {Worldwide {{Soundscapes}}},
  author = {Darras, Kevin Fa and Rountree, Rodney and Van Wilgenburg, Steven and Cord, Anna F and Pitz, Frederik and Chen, Youfang and Dong, Lijun and Gasc, Amandine and Lin, Tzu-Hao and Diaz, Patrick Mauritz and Wu, Shih-Hung and Salton, Marcus and Marley, Sarah and Schill{\'e}, Laura and Wensveen, Paul Jacobus and Desjonqu{\`e}res, Camille and {Acevedo-Charry}, Orlando and Adam, Maty{\'a}{\v s} and Aguzzi, Jacopo and Andr{\'e}, Michel and Antonelli, Alexandre and Aparecido Do Nascimento, Leandro and Appel, Giulliana and Astaras, Christos and Atemasov, Andrey and Barbaro, Luc and Basan, Fritjof and Batist, Carly and Baucells, Adri{\'a} L{\'o}pez and Baumgarten, Julio Ernesto and Bayle Sempere, Just T and Bellisario, Kristen and Ben David, Asaf and {Berger-Tal}, Oded and Betts, Matthew G and Bhalla, Iqbal and Bicudo, Thiago and Bolgan, Marta and Bombaci, Sara and Boullhesen, Martin and {Bradfer-Lawrence}, Tom and Briers, Robert A and Budka, Michal and Burchard, Katie and Calvente, Alice and {Cerezo-Araujo}, Maite and Cerw{\'e}n, Gunnar and Chistopolova, Maria and Clark, Christopher W and Cretois, Benjamin and Czarnecki, Chapin and Da Silva, Luis P and Da Silva, Wigna and De Clippele, Laurence H and De La Haye, David and De Oliveira Tissiani, Ana Silvia and De Zwaan, Devin and {D{\'i}az-Delgado}, Ricardo and Diniz, Pedro and {Di{\'o}genes Oliveira-J{\'u}nior}, Dorgival and Dorigo, Thiago and Dr{\"o}ge, Saskia and Duarte, Marina and Duarte, Adam and Dunleavy, Kerry and Dziak, Robert and Elise, Simon and Enari, Hiroto and Enari, Haruka S and Erbs, Florence and Ferrari, Nina and Ferreira, Luane and Fleishman, Abram B and Freitas, B{\'a}rbara and Friedman, Nick and Froidevaux, J{\'e}r{\'e}my Sp and Gogoleva, Svetlana and Gon{\c c}alves, Maria Isabel and Gonzaga, Carolina and Gonz{\'a}lez Correa, Jos{\'e} Miguel and Goodale, Eben and Gottesman, Benjamin and Grass, Ingo and Greenhalgh, Jack and Gregoire, Jocelyn and Hagge, Jonas and Halliday, William and Hammer, Antonia and {Hanf-Dressler}, Tara and Haver, Samara and Hending, Daniel and {Hernandez-Blanco}, Jose and Hiller, Thomas and Huang, Joe Chun-Chia and Hutchinson, Katie Lois and Jackson, Janet and Jacot, Alain and Jahn, Olaf and Kanes, Jasper and Kenchington, Ellen and {Kepfer-Rojas}, Sebastian and Kitzes, Justin and Kusuminda, Tharaka and Lehnardt, Yael and Leitman, Paula and Leon, Jos{\'e} and {Lima-Santos}, Cicero Sim{\~a}o and Lloyd, Kyle John and Looby, Audrey and {L{\'o}pez-Bosch}, David and Maeda, Tatiana and Mammides, Christos and Marcacci, Gabriel and Markolf, Matthias and Marques, Marinez Isaac and Martin, Charles W and Martin, Dominic A and Martin, Kathy and McKown, Matthew and McLeod, Logan Jt and Metcalf, Oliver and Meyer, Christoph and Mikusinski, Grzegorz and Moreira Sugai, Larissa Sayuri and Morris, Dave and M{\"u}ller, Sandra and Murchy, Kelsie A and Navarro, Maria Mas and Nouioua, Rym and Olden, Julian D and Oppel, Steffen and Osiecka, Anna N and Parsons, Miles and Pereira Samarra, Filipa Isabel and {P{\'e}rez-Granados}, Cristian and Piatti, Liliana and Pichorim, Mauro and Pinheiro, Thiago and Pradervand, Jean-Nicolas and Radford, Craig and Ramalho, Emiliano and Reynolds, Laura K and Riede, Klaus and Rimmer, Talen and Rocha, Ricardo and Rocha, Luciana and Ross, Samuel Rp-J and Rosten, Carolyn and {Salustio-Gomes}, Carlos and Samartzis, Philip and Santos, Jos{\'e} and Scharffenberg, Kevin and Schoeman, Ren{\'e}e P and Schuchmann, Karl-Ludwig and {Sebasti{\'a}n-Gonz{\'a}lez}, Esther and Shabangu, Fannie and Shaw, Taylor and Shen, Xiaoli and Singer, David and Stanley, Jenni and Thomisch, Karolin and Torrent, Laura and Traba, Juan and Tremblay, Junior A and Tseng, Sunny and Tuanmu, Mao-Ning and Valverde, Marisol and Vernasco, Ben and Vital Da Paz, Raiane and Ward, Matthew and Weldy, Matthew and Wiel, Julia and Willie, Jacob and Wood, Heather and Li, Songhai and {Sousa-Lima}, Renata and Wanger, Thomas Cherico},
  year = {2024},
  month = apr,
  doi = {10.1101/2024.04.10.588860},
  urldate = {2024-08-22},
  abstract = {Abstract                        The urgency for remote, reliable, and scalable biodiversity monitoring amidst mounting human pressures on climate and ecosystems has sparked worldwide interest in Passive Acoustic Monitoring (PAM), but there has been no comprehensive overview of its coverage across realms. We present metadata from 358 datasets recorded since 1991 in and above land and water constituting the first global synthesis of sampling coverage across spatial, temporal, and ecological scales. We compiled summary statistics (sampling locations, deployment schedules, focal taxa, and recording parameters) and used eleven case studies to assess trends in biological, anthropogenic, and geophysical sounds. Terrestrial sampling is spatially denser (42 sites/M{$\cdot$}km             2             ) than aquatic sampling (0.2 and 1.3 sites/M{$\cdot$}km             2             in oceans and freshwater) with only one subterranean dataset. Although diel and lunar cycles are well-covered in all realms, only marine datasets (65\%) comprehensively sample all seasons. Across realms, biological sounds show contrasting diel activity, while declining with distance from the equator and anthropogenic activity. PAM can thus inform phenology, macroecology, and conservation studies, but representation can be improved by widening terrestrial taxonomic breadth, expanding coverage in the high seas, and increasing spatio-temporal replication in freshwater habitats. Overall, PAM shows considerable promise to support global biodiversity monitoring efforts.},
  langid = {english},
  file = {C:\Users\daniel\Zotero\storage\SRVJGZWH\Darras et al. - 2024 - Worldwide Soundscapes a synthesis of passive acou.pdf}
}

@misc{WorldwideSoundscapesSynthesis,
  title = {Worldwide {{Soundscapes}}: A Synthesis of Passive Acoustic Monitoring across Realms {\textbar} {{bioRxiv}}},
  urldate = {2024-08-22},
  howpublished = {https://www.biorxiv.org/content/10.1101/2024.04.10.588860v4.abstract}
}

@article{eliseCombiningPassiveAcoustics2022,
  title = {Combining {{Passive Acoustics}} and {{Environmental Data}} for {{Scaling Up Ecosystem Monitoring}}: {{A Test}} on {{Coral Reef Fishes}}},
  shorttitle = {Combining {{Passive Acoustics}} and {{Environmental Data}} for {{Scaling Up Ecosystem Monitoring}}},
  author = {Elise, Simon and Guilhaumon, Fran{\c c}ois and {Mou-Tham}, G{\'e}rard and {Urbina-Barreto}, Isabel and Vigliola, Laurent and Kulbicki, Michel and Bruggemann, J. Henrich},
  year = {2022},
  month = jan,
  journal = {Remote Sensing},
  volume = {14},
  number = {10},
  pages = {2394},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2072-4292},
  doi = {10.3390/rs14102394},
  urldate = {2024-08-22},
  abstract = {Ecological surveys of coral reefs mostly rely on visual data collected by human observers. Although new monitoring tools are emerging, their specific advantages should be identified to optimise their simultaneous use. Based on the goodness-of-fit of linear models, we compared the potential of passive acoustics and environmental data for predicting the structure of coral reef fish assemblages in different environmental and biogeographic settings. Both data types complemented each other. Globally, the acoustic data showed relatively low added value in predicting fish assemblage structures. The predictions were best for the distribution of fish abundance among functional entities (i.e., proxies for fish functional groups, grouping species that share similar eco-morphological traits), for the simplest functional entities (i.e., combining two eco-morphological traits), and when considering diet and the level in the water column of the species. Our study demonstrates that Passive Acoustic Monitoring (PAM) improves fish assemblage assessment when used in tandem with environmental data compared to using environmental data alone. Such combinations can help with responding to the current conservation challenge by improving our surveying capacities at increased spatial and temporal scales, facilitating the identification and monitoring of priority management areas.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {conservation,coral reefs,ecoacoustic indices,fish assemblages,Passive Acoustic Monitoring (PAM),remote sensing},
  file = {C:\Users\daniel\Zotero\storage\C9T5KH73\Elise et al. - 2022 - Combining Passive Acoustics and Environmental Data.pdf}
}

@incollection{coxFishSoundsVersionData2023,
  title = {{{FishSounds Version}} 1.1: {{Data Archive}}, {{User Experience}}, and {{Online Resources}}},
  shorttitle = {{{FishSounds Version}} 1.1},
  booktitle = {The {{Effects}} of {{Noise}} on {{Aquatic Life}} : {{Principles}} and {{Practical Considerations}}},
  author = {Cox, Kieran D. and Looby, Audrey and Vela, Sarah and Riera, Amalis and Bravo, Santiago and Davies, Hailey L. and Rountree, Rodney and Spriel, Brittnie and Reynolds, Laura K. and Martin, Charles W. and Matwin, Stan and Juanes, Francis},
  editor = {Popper, Arthur N. and Sisneros, Joseph and Hawkins, Anthony D. and Thomsen, Frank},
  year = {2023},
  pages = {1--12},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-10417-6_35-1},
  urldate = {2024-08-22},
  abstract = {Fish sound production is taxonomically and geographically widespread. Produced actively or passively, acoustic contributions of fishes to aquatic soundscapes support a myriad of con- and hetero-specific interactions. Despite the ecological importance of fish sounds, the field of bioacoustics historically lacked a global inventory of fish species known to produce sound. FishSounds launched in 2021, cataloging examinations of active and passive sound production by 1185 fish species from 837 references, as well as 239 exemplary audio recordings. In April 2022, FishSounds released Version 1.1 to expand the website's data and improve its functionality. This new version updated the core dataset to include work published up until the end of the year 2021, adding 30 species, 25 recordings, and 85 references. It also added text to note the original species names used by the authors, updated all taxonomy data to FishBase version 02/2022, and included a webpage highlighting other resources that might be of interest to users. Additionally, FishSounds Educate launched a series of interactive lectures, classroom workshops, and field labs for primary, secondary, and university students. Future FishSounds versions will prioritize streamlining the data submission process, improving the user experience, and using bioacoustics to increase ocean literacy.},
  isbn = {978-3-031-10417-6},
  langid = {english}
}

@article{wangAgriPestLargeScaleDomainSpecific2021,
  title = {{{AgriPest}}: {{A Large-Scale Domain-Specific Benchmark Dataset}} for {{Practical Agricultural Pest Detection}} in the {{Wild}}},
  shorttitle = {{{AgriPest}}},
  author = {Wang, Rujing and Liu, Liu and Xie, Chengjun and Yang, Po and Li, Rui and Zhou, Man},
  year = {2021},
  month = feb,
  journal = {Sensors (Basel, Switzerland)},
  volume = {21},
  number = {5},
  pages = {1601},
  issn = {1424-8220},
  doi = {10.3390/s21051601},
  urldate = {2024-08-26},
  abstract = {The recent explosion of large volume of standard dataset of annotated images has offered promising opportunities for deep learning techniques in effective and efficient object detection applications. However, due to a huge difference of quality between these standardized dataset and practical raw data, it is still a critical problem on how to maximize utilization of deep learning techniques in practical agriculture applications. Here, we introduce a domain-specific benchmark dataset, called AgriPest, in tiny wild pest recognition and detection, providing the researchers and communities with a standard large-scale dataset of practically wild pest images and annotations, as well as evaluation procedures. During the past seven years, AgriPest captures 49.7K images of four crops containing 14 species of pests by our designed image collection equipment in the field environment. All of the images are manually annotated by agricultural experts with up to 264.7K bounding boxes of locating pests. This paper also offers a detailed analysis of AgriPest where the validation set is split into four types of scenes that are common in practical pest monitoring applications. We explore and evaluate the performance of state-of-the-art deep learning techniques over AgriPest. We believe that the scale, accuracy, and diversity of AgriPest can offer great opportunities to researchers in computer vision as well as pest monitoring applications.},
  pmcid = {PMC7956390},
  pmid = {33668820},
  file = {C:\Users\daniel\Zotero\storage\35IJD95H\Wang et al. - 2021 - AgriPest A Large-Scale Domain-Specific Benchmark .pdf}
}

@article{ozgurMatLabVsPython2017,
  title = {{{MatLab}} vs. {{Python}} vs. {{R}}},
  author = {Ozgur, Ceyhun and Colliau, Taylor and Rogers, Grace and Hughes, Zachariah and Bennie, Elyse},
  year = {2017},
  month = jul,
  journal = {Journal of data science: JDS},
  volume = {15},
  pages = {355--372},
  abstract = {Matlab, Python and R have all been used successfully in teaching college students fundamentals of mathematics \& statistics. In today's data driven environment, the study of data through big data analytics is very powerful, especially for the purpose of decision making and using data statistically in this data rich environment. MatLab can be used to teach introductory mathematics such as calculus and statistics. Both Python and R can be used to make decisions involving big data. On the one hand, Python is perfect for teaching introductory statistics in a data rich environment. On the other hand, while R is a little more involved, there are many customizable programs that can make somewhat involved decisions in the context of prepackaged, preprogrammed statistical analysis.}
}

@misc{TIOBEIndex,
  title = {{{TIOBE Index}}},
  journal = {TIOBE},
  urldate = {2024-08-27},
  howpublished = {https://www.tiobe.com/tiobe-index/},
  langid = {american},
  file = {C:\Users\daniel\Zotero\storage\AB8E7V62\tiobe-index.html}
}

@inproceedings{sedunovStevensDroneDetection2019a,
  title = {Stevens {{Drone Detection Acoustic System}} and {{Experiments}} in {{Acoustics UAV Tracking}}},
  booktitle = {2019 {{IEEE International Symposium}} on {{Technologies}} for {{Homeland Security}} ({{HST}})},
  author = {Sedunov, Alexander and Haddad, Darren and Salloum, Hady and Sutin, Alexander and Sedunov, Nikolay and Yakubovskiy, Alexander},
  year = {2019},
  month = nov,
  pages = {1--7},
  doi = {10.1109/HST47167.2019.9032916},
  urldate = {2024-08-28},
  abstract = {Concerns about improper use of Unmanned Aerial Vehicles (UAVs, also unmanned aerial systems or UAS) led to the development of various methods for their detection, tracking, and classification. One of the methods is using acoustics. Advantages of passive acoustics include the low cost of acoustic systems, finding of the Direction of Arrival (DOA) and localization by simple means; and classification of UAV sounds by acoustic signature. Stevens Institute of Technology has developed and built the Drone Acoustic Detection System (DADS) that can detect, track and classify UAVs based on the propeller noise. The system consists of three or more microphone nodes. The microphones in each node are arranged in a tetrahedron with configurable size. The microphone data is transferred over WiFi in real-time to a central computer, where it is processed. The Stevens DADS system was investigated in numerous field tests together with several other directional arrays including a 16-channel two-tier cross array, the OptiNav 40-microphone phased array, and parabolic and shot gun microphones. Multirotor UAVs of different sizes were employed in testing, including DJI models Phantom 4, M600 and S1000. The test results of the different systems were compared.},
  keywords = {and classification,passive acoustics,signal processing,UAV detection tracking},
  file = {C\:\\Users\\daniel\\Zotero\\storage\\SCU7637H\\Sedunov et al. - 2019 - Stevens Drone Detection Acoustic System and Experi.pdf;C\:\\Users\\daniel\\Zotero\\storage\\EKQ3CUF2\\9032916.html}
}
